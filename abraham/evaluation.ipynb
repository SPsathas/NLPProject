{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from metrics.ExactMatch import ExactMatch\n",
    "\n",
    "def rename_key(data_dict, old_key, new_key):\n",
    "    if old_key in data_dict:\n",
    "        data_dict[new_key] = data_dict.pop(old_key)\n",
    "    else:\n",
    "        print(f\"Key '{old_key}' not found in the dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load a CSV file into a dictionary\n",
    "# the dictionary keys are the column headers and the values are lists of column data\n",
    "def load_csv_to_dict(file_path, name, delimiter=','):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter=delimiter)\n",
    "        \n",
    "        data_dict = {field: [] for field in csv_reader.fieldnames}\n",
    "        data_dict['name'] = name  # Add the experiment name to the dictionary\n",
    "\n",
    "        for row in csv_reader:\n",
    "            for field in csv_reader.fieldnames:\n",
    "                data_dict[field].append(row[field])\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "# function tofind all results.csv files\n",
    "# returns a list of tuples containing the parent directory name and the path to the results.csv file\n",
    "def load_results_csv_paths(root_dir):\n",
    "    results_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for dir_name in dirs:\n",
    "            csv_dir_path = os.path.join(root, dir_name, 'csv')\n",
    "            results_path = os.path.join(csv_dir_path, 'results.csv')\n",
    "            \n",
    "            if os.path.isfile(results_path):\n",
    "                parent_dir = os.path.basename(root)\n",
    "                results_list.append((parent_dir, results_path))\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(sentence1, sentence2):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two sentences using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "    sentence1 (str): The first sentence.\n",
    "    sentence2 (str): The second sentence.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity score between the two sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a TF-IDF vectorizer\n",
    "\n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transform the sentences into TF-IDF vectors\n",
    "    tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
    "\n",
    "    # Compute the cosine similarity between the sentences\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    # Print the similarity score\n",
    "    similarity_score = similarity_matrix[0][0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "def exact_match(prediction, reference) -> float:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    exact_match_metric = ExactMatch()\n",
    "    results = exact_match_metric.evaluate(prediction, reference)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def scores(responde_file, K):\n",
    "    \"\"\"\n",
    "    Calculate the BERT similarity scores between correct and predicted answers,\n",
    "    and create a new CSV file with these scores.\n",
    "\n",
    "    :param question: The list of questions corresponding to the answers\n",
    "    :param correct_answers: A list of correct answers\n",
    "    :param predicted_answers: A list of predicted answers\n",
    "    :param evaluation_csv_path: Path to the CSV file to be created\n",
    "    :param K: Column name for the similarity scores in the CSV\n",
    "    \"\"\"\n",
    "    questions = responde_file['Prompt']\n",
    "    predicted_answers = responde_file['GPT Response']\n",
    "    correct_answers = responde_file['Ground Truth Answer']\n",
    "    # Load the pre-trained Sentence-BERT model\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    # Check if the input lists have the same length\n",
    "    if len(correct_answers) != len(predicted_answers):\n",
    "        raise ValueError(\"The lists of correct and predicted answers must have the same length\")\n",
    "\n",
    "    # Prepare data for the DataFrame\n",
    "    data_list = []\n",
    "    index = 0\n",
    "    # Compute the similarity score for each pair of answers\n",
    "    for correct, predicted, question in zip(correct_answers, predicted_answers, questions):\n",
    "        # Encode the sentences to get their embeddings\n",
    "        embedding1 = model.encode(correct, convert_to_tensor=True)\n",
    "        embedding2 = model.encode(predicted, convert_to_tensor=True)\n",
    "\n",
    "        # Compute the cosine similarity between the embeddings\n",
    "        bert_score = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "        cosine_score = compute_cosine_similarity(correct, predicted)\n",
    "        exact_match_score = exact_match(correct, predicted)\n",
    "        # Collect data\n",
    "        data_list.append({'index': index,\n",
    "                          'question': question,\n",
    "                          'predicted_answer': predicted,\n",
    "                          'actual_answer': correct,\n",
    "                          'bert_score': bert_score,\n",
    "                          'cosine_score': cosine_score,\n",
    "                          'exact_match': exact_match_score\n",
    "                          })\n",
    "        index += 1\n",
    "    # Create a DataFrame from collected data\n",
    "    data = pd.DataFrame(data_list)\n",
    "\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_bert_scores(file_path1, file_path2):\n",
    "    \"\"\"\n",
    "    Calculate the BERT scores between corresponding predicted answers in two result files.\n",
    "\n",
    "    :param file_path1: Path to the first CSV file (results_retrival_1.csv).\n",
    "    :param file_path2: Path to the second CSV file (results_retrival_3.csv).\n",
    "    :return: DataFrame with the BERT scores for corresponding rows.\n",
    "    \"\"\"\n",
    "    # Load the CSV files\n",
    "    df1 = pd.read_csv(file_path1)\n",
    "    df2 = pd.read_csv(file_path2)\n",
    "\n",
    "    # Load the pre-trained Sentence-BERT model\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    # Check if the files have the same number of rows\n",
    "    if len(df1) != len(df2):\n",
    "        raise ValueError(\"Both files must have the same number of rows for 1-to-1 comparison.\")\n",
    "\n",
    "    # List to hold the score data\n",
    "    scores_list = []\n",
    "\n",
    "    # Iterate over the rows in both DataFrames by index\n",
    "    for index, (predicted1, predicted2) in enumerate(zip(df1['predicted_answer'], df2['predicted_answer'])):\n",
    "        # Encode the predicted answers to get their embeddings\n",
    "        embedding1 = model.encode(predicted1, convert_to_tensor=True)\n",
    "        embedding2 = model.encode(predicted2, convert_to_tensor=True)\n",
    "\n",
    "        # Compute the cosine similarity between the embeddings\n",
    "        bert_score = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "\n",
    "        # Append the results to the list\n",
    "        scores_list.append({\n",
    "            'Index': index,\n",
    "            'Predicted_Answer_File1': predicted1,\n",
    "            'Predicted_Answer_File2': predicted2,\n",
    "            'BERT_Score': bert_score\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from collected data\n",
    "    result_df = pd.DataFrame(scores_list)\n",
    "    result_df.to_csv(f'data/results_cross_scores.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CELL BELOW IS JUST FOR THE RANDOM EXPERIMENTS (3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: top10-random-k1_3-k2_2_middle\n",
      "Number of Ground Truth Answers: 1199\n",
      "First GPT Response: Dorota Masłowska\n",
      "First Question ID: 8813f87c0bdd11eba7f7acde48001122\n",
      "First Prompt:\n",
      " Kitty Marion\n",
      "Kitty Marion (12 March 1871 – 9 October 1944) was born Katherina Maria Schäfer in Germany. She immigrated to London in 1886 when she was fifteen, and she grew to minor prominence when she sang in music halls throughout the United Kingdom during the late 19th century. She became known in the field for standing up for female performers against agents, corruption, and for better working conditions. She joined the Women’s Social and Political Union (WSPU) in 1908, engaged in selling their newspaper \"Votes for Women\" and became a prominent suffragette in the United Kingdom for her participation in civil unrest protests including riots and arson. As a result, Marion was arrested many times and is known for having endured 232 force-feedings while on hunger strike in prison. She is quoted as saying “there are no words to describe the horrible revolting sensation.” When World War I started she emigrated to the United States, and there she joined the team on Margaret Sanger’s \"Birth Control Review\". Although she used her tenacity and loud voice to get people to pay attention to her cause, she did not use violence as much as she had in the United Kingdom, although she was still arrested many times for advocating birth control.\n",
      "\n",
      "Russian State Social University\n",
      "Russian State Social University (RSSU; Russian: Российский государственный социальный университет, abbreviated as РГСУ) was the first public university in the Russian Federation to offer undergraduate and graduate programmes in the field of social work. It is located in Moscow where positioned its three main historically important campuses. Russian State Social University is recognized as a fully accredited, state-owned, traditional institution. The current rector is Natalia Pochinok.\n",
      "\n",
      "Polish-Russian War (film)\n",
      "Polish- Russian War( Wojna polsko- ruska) is a 2009 Polish film directed by Xawery Żuławski based on the novel Polish- Russian War under the white- red flag by Dorota Masłowska.\n",
      "\n",
      "Sitora Alieva\n",
      "Sitora Shokhinovna Alieva – film expert, director of the IFF “ Faces of love ” and the IIF Sochi, artistic director of the largest Russian national film festival “ Kinotavr ”, Russian Ministry of Culture film expert, lecturer at film schools and universities, juror at numerous film festivals, including Berlinale, Venice Film Festival, etc.\n",
      "\n",
      "The Last Family\n",
      "The Last Family  is a 2016 Polish biographical film directed by Jan P. Matuszyński. The film won the Golden Lions for best film at the 2016 Gdynia Film Festival.\n",
      "\n",
      "Gojeb River\n",
      "The Gojeb River is eastward-flowing tributary of the Omo River in Ethiopia. It rises in the mountains of Guma, flowing in almost a direct line its confluence with the Omo at\n",
      "\n",
      "Ann Arbor, Michigan\n",
      "A person from Ann Arbor is called an \"Ann Arborite\", and many long-time residents call themselves \"townies\". The city itself is often called \"A²\" (\"A-squared\") or \"A2\" (\"A two\") or \"AA\", \"The Deuce\" (mainly by Chicagoans), and \"Tree Town\". With tongue-in-cheek reference to the city's liberal political leanings, some occasionally refer to Ann Arbor as \"The People's Republic of Ann Arbor\" or \"25 square miles surrounded by reality\", the latter phrase being adapted from Wisconsin Governor Lee Dreyfus's description of Madison, Wisconsin. In A Prairie Home Companion broadcast from Ann Arbor, Garrison Keillor described Ann Arbor as \"a city where people discuss socialism, but only in the fanciest restaurants.\" Ann Arbor sometimes appears on citation indexes as an author, instead of a location, often with the academic degree MI, a misunderstanding of the abbreviation for Michigan. Ann Arbor has become increasingly gentrified in recent years.\n",
      "\n",
      "Who is the mother of the director of film Polish-Russian War (Film)?\n"
     ]
    }
   ],
   "source": [
    "# set the root directory to start searching for results.csv files, should be the folder containing the results folder\n",
    "root_directory = '.'\n",
    "results_list = load_results_csv_paths(root_directory)\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "# load each results.csv file into a dictionary and add it to csv_list\n",
    "for name, path in results_list:\n",
    "    results = load_csv_to_dict(path, name)\n",
    "    csv_list.append(results)\n",
    "\n",
    "# now csv_list contains a list of dictionaries, each dictionary representing a CSV file\n",
    "# the fields in each dictionary are 'name' (experiment description) and the CSV headers\n",
    "\n",
    "# example print statements to show some data from the frist csv dictionary in csv_list\n",
    "# print(\"Experiment name:\", csv_list[0]['name'])\n",
    "# print(\"Number of Ground Truth Answers:\", len(csv_list[0]['Ground Truth Answer']))\n",
    "# print(\"First GPT Response:\", csv_list[0]['GPT Response'][0])\n",
    "# print(\"First Question ID:\", csv_list[0]['Question ID'][0])\n",
    "# print(\"First Prompt:\\n\", csv_list[0]['Prompt'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPUTE SCORES FOR THE RANDOM EXPERIMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dir for scores\n",
    "os.makedirs('scores', exist_ok=True)\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    data = scores(csv_file, 0)\n",
    "    data.to_csv(f'scores/{csv_file[\"name\"]}_scores.csv', index=False)\n",
    "    print(f'scores for {csv_file[\"name\"]} saved to scores/{csv_file[\"name\"]}_scores.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD CSV FILES OF EARLIER EXPERIMENTS (0), (1), (2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1_qa_topk_k3\n",
      "e1_qa_topk_k1\n",
      "e2_oracle\n",
      "e1_qa_topk_k5\n",
      "e0_qa_output_no_context\n",
      "\n",
      "\n",
      "dict_keys(['Prompt', 'name', 'GPT Response', 'Ground Truth Answer'])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(earlier_dicts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m earlier_dicts:\n\u001b[0;32m---> 21\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_scores.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved to scores/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_scores.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 55\u001b[0m, in \u001b[0;36mscores\u001b[0;34m(responde_file, K)\u001b[0m\n\u001b[1;32m     53\u001b[0m correct_answers \u001b[38;5;241m=\u001b[39m responde_file[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth Answer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Load the pre-trained Sentence-BERT model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparaphrase-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Check if the input lists have the same length\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(correct_answers) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(predicted_answers):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "path = 'results/_earlier_experiments'\n",
    "\n",
    "# load every csv file in the directory\n",
    "\n",
    "earlier_dicts = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(path, file)\n",
    "        file_name = file.split('.')[0]\n",
    "        _dict = load_csv_to_dict(file_path, file_name, delimiter=';')\n",
    "        rename_key(_dict, 'Response', 'GPT Response')\n",
    "        rename_key(_dict, 'Ground Truth', 'Ground Truth Answer')\n",
    "        earlier_dicts.append(_dict)\n",
    "        print(_dict['name'])\n",
    "print('\\n')\n",
    "        \n",
    "print(earlier_dicts[0].keys())\n",
    "\n",
    "for csv_file in earlier_dicts:\n",
    "    data = scores(csv_file, 0)\n",
    "    data.to_csv(f'scores/{csv_file[\"name\"]}_scores.csv', index=False)\n",
    "    print(f'scores for {csv_file[\"name\"]} saved to scores/{csv_file[\"name\"]}_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADORE (4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1_adore_k5\n",
      "e1_adore_k3\n",
      "e1_adore_k1\n",
      "\n",
      "\n",
      "dict_keys(['Prompt', 'name', 'GPT Response', 'Ground Truth Answer'])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(earlier_dicts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m earlier_dicts:\n\u001b[0;32m---> 21\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_scores.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved to scores/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_scores.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 55\u001b[0m, in \u001b[0;36mscores\u001b[0;34m(responde_file, K)\u001b[0m\n\u001b[1;32m     53\u001b[0m correct_answers \u001b[38;5;241m=\u001b[39m responde_file[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth Answer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Load the pre-trained Sentence-BERT model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparaphrase-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Check if the input lists have the same length\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(correct_answers) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(predicted_answers):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "path = 'results/_adore'\n",
    "\n",
    "# load every csv file in the directory\n",
    "\n",
    "earlier_dicts = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(path, file)\n",
    "        file_name = file.split('.')[0]\n",
    "        _dict = load_csv_to_dict(file_path, file_name, delimiter=';')\n",
    "        rename_key(_dict, 'Response', 'GPT Response')\n",
    "        rename_key(_dict, 'Ground Truth', 'Ground Truth Answer')\n",
    "        earlier_dicts.append(_dict)\n",
    "        print(_dict['name'])\n",
    "print('\\n')\n",
    "        \n",
    "print(earlier_dicts[0].keys())\n",
    "\n",
    "for csv_file in earlier_dicts:\n",
    "    data = scores(csv_file, 0)\n",
    "    data.to_csv(f'scores/{csv_file[\"name\"]}_scores.csv', index=False)\n",
    "    print(f'scores for {csv_file[\"name\"]} saved to scores/{csv_file[\"name\"]}_scores.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
