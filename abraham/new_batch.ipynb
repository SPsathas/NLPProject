{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import openai\n",
    "import random\n",
    "from typing import Any\n",
    "from httpx import HTTPStatusError\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import backoff\n",
    "import os\n",
    "import uuid\n",
    "import hashlib\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def load_json(path: str) -> Any:\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def save_json(path: str, data: Any) -> None:\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk(file_name):\n",
    "    match = re.search(r'part_(\\d+)_rslt\\.csv', file_name)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "def concatenate_csv_files(root_dir):\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        csv_files = [f for f in files if f.endswith('.csv') and f != 'results.csv']\n",
    "        \n",
    "        if csv_files:\n",
    "            csv_files.sort(key=sk)\n",
    "            print(csv_files)\n",
    "            results_file_path = os.path.join(subdir, 'results.csv')\n",
    "            \n",
    "            with open(results_file_path, 'w', newline='') as results_file:\n",
    "                writer = csv.writer(results_file)\n",
    "                header_written = False\n",
    "                \n",
    "                for file_name in csv_files:\n",
    "                    file_path = os.path.join(subdir, file_name)\n",
    "                    with open(file_path, 'r') as csv_file:\n",
    "                        reader = csv.reader(csv_file)\n",
    "                        header = next(reader)\n",
    "                        \n",
    "                        if not header_written:\n",
    "                            writer.writerow(header)\n",
    "                            header_written = True\n",
    "                        \n",
    "                        for row in reader:\n",
    "                            writer.writerow(row)\n",
    "                \n",
    "            with open(results_file_path, 'r') as results_file:\n",
    "                reader = csv.reader(results_file)\n",
    "                print(f'{results_file_path}: {sum(1 for _ in reader)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevQuestions:\n",
    "    class Question:\n",
    "        def __init__(self, identifier: str, question: str, answer: str):\n",
    "            \"\"\"\n",
    "            initialize a Question instance.\n",
    "\n",
    "            Args:\n",
    "                identifier (str): the identifier for the question.\n",
    "                question (str): the question text.\n",
    "                answer (str): the answer text.\n",
    "            \"\"\"\n",
    "            self.id = identifier\n",
    "            self.question = question\n",
    "            self.answer = answer\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"\n",
    "        initialize the DevQuestions instance by loading questions from a JSON file.\n",
    "\n",
    "        Args:\n",
    "            path (str): the file path to the JSON file containing the questions.\n",
    "        \"\"\"\n",
    "        # load json file\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.questions = {}\n",
    "        self.lut = []\n",
    "\n",
    "        for question in data:\n",
    "            self.lut.append(question['_id'])\n",
    "            self.questions[question['_id']] = self.Question(question['_id'], question['question'], question['answer'])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        return the number of questions.\n",
    "\n",
    "        Returns:\n",
    "            int: the number of questions.\n",
    "        \"\"\"\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, key: int | str):\n",
    "        \"\"\"\n",
    "        retrieve a question by index or id.\n",
    "\n",
    "        Args:\n",
    "            key (int | str): the idx or id of the question.\n",
    "\n",
    "        Returns:\n",
    "            DevQuestions.Question: the question corresponding to the given index or id.\n",
    "        \"\"\"\n",
    "        if isinstance(key, int):\n",
    "            return self.questions.get(self.lut[key])\n",
    "        elif isinstance(key, str):\n",
    "            return self.questions[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA:\n",
    "    def __init__(self, path_to_dev: str, model: str = 'gpt-3.5-turbo', system_prompt: str = '', actually_prompt: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize a QA instance.\n",
    "\n",
    "        Args:\n",
    "            path_to_dev (str): The file path to the development question.\n",
    "            model (str, optional): The model to use. Defaults to 'gpt-3.5-turbo'.\n",
    "            system_prompt (str, optional): The system prompt to use. Defaults to ''.\n",
    "            actually_prompt (bool, optional): Whether to actually prompt the model. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.client = openai.Client()\n",
    "        self.system_prompt = system_prompt\n",
    "        self.actually_prompt = actually_prompt\n",
    "        self.dev = DevQuestions(path_to_dev)\n",
    "        self.contexts = {}\n",
    "        self.model = model\n",
    "\n",
    "    def batch_by_context(self, context_name: str, location: str, file_name: str, num_questions: int = 0, k: int = 0, write_debug_file: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Ask questions by context and save to a JSONL file.\n",
    "\n",
    "        Args:\n",
    "            context_name (str): The name of the context.\n",
    "            path (str): The file path to save the JSONL file.\n",
    "            num_questions (int, optional): The number of questions to ask. Defaults to 0 (all questions).\n",
    "            k (int, optional): The number of contexts to include in the prompt. Defaults to 0 (all available contexts).\n",
    "            write_to_file (tuple, optional): A tuple with a boolean and a file path to write the prompts to a text file.\n",
    "\n",
    "        Returns:\n",
    "            str: The path to the JSONL file containing the batched requests.\n",
    "        \"\"\"\n",
    "        if context_name not in self.contexts:\n",
    "            raise ValueError('Context not found!')\n",
    "        \n",
    "        num_questions = self._validate_num_questions(num_questions, context_name)\n",
    "\n",
    "        batch = {}\n",
    "        k_adjustments = 0\n",
    "        prompts = []\n",
    "        q_ids = []\n",
    "\n",
    "        for c, (q_id, contexts) in enumerate(self.contexts[context_name].items()):\n",
    "            if c == num_questions:\n",
    "                break\n",
    "\n",
    "            k_, k_adjustments = self._adjust_k(k, contexts, k_adjustments)\n",
    "            contexts = contexts[:k_]\n",
    "\n",
    "            prompt = self._create_prompt(contexts, self.dev[q_id].question)\n",
    "            prompts.append(prompt)\n",
    "            q_ids.append(q_id)\n",
    "            batch[q_id] = self._build_request_dict(q_id, prompt)\n",
    "\n",
    "        self._print_k_adjustments_warning(k_adjustments, len(batch))\n",
    "        self._save_to_jsonl(batch, location, file_name)\n",
    "        \n",
    "        if write_debug_file:\n",
    "            self._write_prompts_to_file(q_ids, prompts, location, file_name)\n",
    "\n",
    "        return (location, file_name)\n",
    "\n",
    "    def batch_by_structured_mixed_context(self, context_name1: str, context_name2: str, path: str, num_questions: int = 0, k1: int = 0, k2: int = 0, position: str = \"first\", write_debug_file: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Ask questions by using the ids from context_name1 and combine contexts from context_name2 in a structured manner,\n",
    "        then save to a JSONL file.\n",
    "\n",
    "        Args:\n",
    "            context_name1 (str): The name of the first context.\n",
    "            context_name2 (str): The name of the second context.\n",
    "            path (str): The directory path to save the JSONL file.\n",
    "            num_questions (int, optional): The number of questions to ask. Defaults to 0 (all questions).\n",
    "            k1 (int, optional): The number of contexts to include from the first context dictionary. Defaults to 0 (all available contexts).\n",
    "            k2 (int, optional): The number of contexts to include from the second context dictionary. Defaults to 0 (all available contexts, doubled).\n",
    "            position (str, optional): The position to place contexts from context_name1 in relation to context_name2. Can be \"first\", \"middle\", or \"last\".\n",
    "            write_debug_file (bool, optional): Whether to write the prompts to a debug file. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            str: The path to the JSONL file containing the batched requests.\n",
    "        \"\"\"\n",
    "        if set(self.contexts[context_name1].keys()) != set(self.contexts[context_name2].keys()):\n",
    "            raise ValueError('Context keys do not match!')\n",
    "\n",
    "        if context_name1 not in self.contexts or context_name2 not in self.contexts:\n",
    "            raise ValueError('Context not found!')\n",
    "\n",
    "        num_questions = self._validate_num_questions(num_questions, context_name1)\n",
    "\n",
    "        print(f\"Creating batch for {num_questions} questions from '{context_name1}' and '{context_name2}' contexts...\")\n",
    "\n",
    "        location = f'{path}/{context_name1}-{context_name2}-k1_{k1}-k2_{k2}_{position}/{self._generate_date_string()}/'\n",
    "\n",
    "        try:\n",
    "            os.makedirs(location, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory '{location}': {e}\")\n",
    "\n",
    "        batch = {}\n",
    "        k1_adjustments = 0\n",
    "        k2_adjustments = 0\n",
    "        prompts = []\n",
    "        q_ids = []\n",
    "\n",
    "        for c, (q_id, contexts1) in enumerate(self.contexts[context_name1].items()):\n",
    "            if c == num_questions:\n",
    "                break\n",
    "\n",
    "            question = self.dev[q_id]\n",
    "            contexts2 = self.contexts[context_name2].get(q_id, [])\n",
    "\n",
    "            k1_, k1_adjustments = self._adjust_k(k1, contexts1, k1_adjustments)\n",
    "            k2_, k2_adjustments = self._adjust_k(k2, contexts2, k2_adjustments)\n",
    "\n",
    "            combined_contexts = self._combine_contexts(contexts1[:k1_], contexts2[:k2_*2], position)\n",
    "            prompt = self._create_prompt(combined_contexts, question.question)\n",
    "            prompts.append(prompt)\n",
    "            q_ids.append(q_id)\n",
    "            batch[q_id] = self._build_request_dict(q_id, prompt)\n",
    "\n",
    "            self._print_k_adjustments_warning(k1_adjustments, len(batch), 'k1')\n",
    "            self._print_k_adjustments_warning(k2_adjustments, len(batch), 'k2')\n",
    "            self._save_to_jsonl(batch, location, 'submitted_batch')\n",
    "\n",
    "        if write_debug_file: self._write_prompts_to_file(q_ids, prompts, location, 'debug')\n",
    "\n",
    "        return f'{location}submitted_batch.jsonl'\n",
    "        \n",
    "    def submit_batch(self, batch_path: str, description: str) -> str:\n",
    "        \"\"\"\n",
    "        Submit a batch of requests to the OpenAI API for asynchronous processing and save the batch ID and description to a CSV file.\n",
    "\n",
    "        args:\n",
    "            jsonl_file_path (str): the path to the batch JSONL file.\n",
    "            description (str): description of the batch job.\n",
    "\n",
    "        returns:\n",
    "            str: The batch job ID.\n",
    "        \"\"\"\n",
    "        batch_input_file = self.client.files.create(\n",
    "            file=open(batch_path, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        batch_input_file_id = batch_input_file.id\n",
    "\n",
    "        batch_job = self.client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"file_name\": batch_path}\n",
    "        )\n",
    "        batch_job_id = batch_job.id\n",
    "\n",
    "        print(f\"Batch job '{batch_path}' submitted with ID: {batch_job_id}\")\n",
    "        \n",
    "        with open('batches.csv', 'a', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow([batch_job_id, description, batch_path])\n",
    "        \n",
    "        return batch_job_id\n",
    "    \n",
    "    def submit_split_batch(self, batch_path: str, description: str, num_splits: int) -> list:\n",
    "        \"\"\"\n",
    "        Split a JSONL file into smaller parts, submit each part as a separate batch, and save the batch IDs and descriptions to a CSV file.\n",
    "\n",
    "        args:\n",
    "            batch_path (str): the path to the batch JSONL file.\n",
    "            description (str): description of the batch job.\n",
    "            num_splits (int): the number of parts to split the JSONL file into.\n",
    "\n",
    "        returns:\n",
    "            list: A list of batch job IDs.\n",
    "        \"\"\"\n",
    "        split_file_paths = self.split_jsonl_file(batch_path, num_splits)\n",
    "        batch_job_ids = []\n",
    "\n",
    "        for index, split_file_path in enumerate(split_file_paths):\n",
    "            split_description = f\"{description} -part {index + 1}\"\n",
    "            batch_job_id = self.submit_batch(split_file_path, split_description)\n",
    "            batch_job_ids.append(batch_job_id)\n",
    "            while not self.is_batch_done(batch_job_id) and not self.is_batch_failed(batch_job_id):\n",
    "                time.sleep(30)\n",
    "\n",
    "            with open(f'{os.path.dirname(batch_path)}/ids.txt', 'a') as f:\n",
    "                f.write(f'{batch_job_id}\\n')\n",
    "\n",
    "        return batch_job_ids\n",
    "    \n",
    "    def split_jsonl_file(self, jsonl_file_path: str, num_splits: int) -> list:\n",
    "        \"\"\"\n",
    "        Split a JSONL file into smaller parts.\n",
    "\n",
    "        args:\n",
    "            jsonl_file_path (str): The path to the JSONL file to split.\n",
    "            num_splits (int): The number of parts to split the file into.\n",
    "\n",
    "        returns:\n",
    "            list: A list of paths to the split JSONL files.\n",
    "        \"\"\"\n",
    "        base_dir = os.path.dirname(jsonl_file_path)\n",
    "        base_name = os.path.basename(jsonl_file_path).split('.')[0]\n",
    "        with open(jsonl_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        total_lines = len(lines)\n",
    "        lines_per_split = total_lines // num_splits\n",
    "        split_file_paths = []\n",
    "\n",
    "        for i in range(num_splits):\n",
    "            split_file_name = f\"{base_name}_part_{i + 1}.jsonl\"\n",
    "            split_file_path = os.path.join(base_dir, split_file_name)\n",
    "            split_lines = lines[i*lines_per_split:(i+1)* lines_per_split] if i < num_splits-1 else lines[i*lines_per_split:]\n",
    "            with open(split_file_path, 'w') as split_file:\n",
    "                split_file.writelines(split_lines)\n",
    "            split_file_paths.append(split_file_path)\n",
    "\n",
    "        return split_file_paths\n",
    "    \n",
    "    def is_batch_done(self, batch_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        check if a batch job is completed\n",
    "\n",
    "        args:\n",
    "            batch_id (str): The ID of the batch job to check.\n",
    "\n",
    "        returns:\n",
    "            bool: True if the batch job is completed, otherwise False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_status = self.client.batches.retrieve(batch_id)\n",
    "            status = batch_status.status\n",
    "            return status == \"completed\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking batch job {batch_id}: {e}\")\n",
    "            return False\n",
    "        \n",
    "    def is_batch_failed(self, batch_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        check if a batch job has failed\n",
    "\n",
    "        args:\n",
    "            batch_id (str): The ID of the batch job to check.\n",
    "\n",
    "        returns:\n",
    "            bool: True if the batch job has failed, otherwise False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_status = self.client.batches.retrieve(batch_id)\n",
    "            status = batch_status.status\n",
    "            return status in [\"failed\", \"expired\", \"canceled\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking batch job {batch_id}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_batch_job(self, batch_job_id: str, csv_file: str = 'batches.csv') -> dict:\n",
    "        \"\"\"\n",
    "        Load batch job details from a CSV file using the batch job ID.\n",
    "\n",
    "        args:\n",
    "            batch_job_id (str): The ID of the batch job to look up.\n",
    "            csv_file (str, optional): The path to the CSV file containing batch job details. Defaults to 'batches.csv'.\n",
    "\n",
    "        returns:\n",
    "            dict: A dictionary containing the batch job details.\n",
    "        \"\"\"\n",
    "        with open(csv_file, 'r', newline='') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            for row in csvreader:\n",
    "                if row[0] == batch_job_id:\n",
    "                    return {\n",
    "                        \"batch_job_id\": row[0],\n",
    "                        \"description\": row[1],\n",
    "                        \"jsonl_file_path\": row[2]\n",
    "                    }\n",
    "        raise ValueError(f\"batch job id '{batch_job_id}' not found in csvv.\")\n",
    "\n",
    "    def check_batches(self, batch_ids: list, csv_file: str = 'batches.csv') -> dict:\n",
    "        \"\"\"\n",
    "        Check the status of multiple batch jobs and save the results if the jobs are completed.\n",
    "\n",
    "        args:\n",
    "            batch_ids (list): A list of batch job IDs to check.\n",
    "            csv_file (str, optional): The path to the CSV file containing batch job details. Defaults to 'batches.csv'.\n",
    "\n",
    "        returns:\n",
    "            dict: A dictionary where the keys are batch job IDs and the values are lists of dictionaries containing the results from the completed batches.\n",
    "        \"\"\"\n",
    "        all_results = {}\n",
    "\n",
    "        for batch_id in batch_ids:\n",
    "            batch_details = self.load_batch_job(batch_id, csv_file)\n",
    "            jsonl_file_path = batch_details[\"jsonl_file_path\"]\n",
    "            save_results_to = os.path.dirname(jsonl_file_path)\n",
    "            batch_name = os.path.basename(jsonl_file_path).split('.')[0]\n",
    "\n",
    "            try:\n",
    "                batch_status = self.client.batches.retrieve(batch_id)\n",
    "                status = batch_status.status\n",
    "                \n",
    "                if status == \"completed\":\n",
    "                    results_file_id = batch_status.output_file_id\n",
    "                    results_file = self.client.files.content(results_file_id)\n",
    "\n",
    "                    results_file_path = os.path.join(save_results_to, f\"{batch_name}_rslt.jsonl\")\n",
    "                    results = []\n",
    "                    with open(results_file_path, 'wb') as output_file:\n",
    "                        for line in results_file.iter_lines():\n",
    "                            if line.strip():\n",
    "                                json_line = json.loads(line)\n",
    "                                results.append(json_line)\n",
    "                                output_file.write((json.dumps(json_line) + '\\n').encode('utf-8'))\n",
    "\n",
    "                    print(f\"batch job results for {batch_id} - {jsonl_file_path} saved to {results_file_path}\")\n",
    "\n",
    "                    try:\n",
    "                        os.makedirs(os.path.join(save_results_to, 'csv'), exist_ok=True)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error creating directory '{save_results_to}/csv': {e}\")\n",
    "\n",
    "                    results_file_path = os.path.join(save_results_to, f\"csv/{batch_name}_rslt.csv\")\n",
    "                    self.write_csv(jsonl_file_path, results_file_path, results)\n",
    "                    all_results[batch_id] = results\n",
    "                elif status in [\"failed\", \"expired\", \"canceled\"]:\n",
    "                    print(f\"batch job {batch_id} / {jsonl_file_path} failed with status: {status}\")\n",
    "                else:\n",
    "                    print(f\"batch job {batch_id} / {jsonl_file_path} is not yet completed. Current status: {status}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"errror checking batch job {batch_id} / {jsonl_file_path}: {e}\")\n",
    "        \n",
    "        return all_results \n",
    "\n",
    "    def set_system_prompt(self, prompt: str):\n",
    "        \"\"\"\n",
    "        sets the system prompt for gpt to use\n",
    "\n",
    "        args:\n",
    "            prompt (str): the prompt to set.\n",
    "        \"\"\"\n",
    "        if not isinstance(prompt, str):\n",
    "            raise TypeError('prompt must be a string!')\n",
    "        self.system_prompt = prompt\n",
    "\n",
    "    def get_context(self, name: str) -> dict:\n",
    "        \"\"\"\n",
    "        get a context by name\n",
    "\n",
    "        args:\n",
    "            name (str): the name of the context.\n",
    "\n",
    "        returns:\n",
    "            dict: the context dictionary.\n",
    "        \"\"\"\n",
    "        if not isinstance(name, str) or name not in self.contexts:\n",
    "            raise ValueError('Context not found!')\n",
    "        return self.contexts.get(name)\n",
    "\n",
    "    def print_context_names(self):\n",
    "        \"\"\"\n",
    "        print the names of all contexts available\n",
    "        \"\"\"\n",
    "        for name in self.contexts.keys():\n",
    "            print(name)\n",
    "\n",
    "    def add_context(self, context: dict, name: str):\n",
    "        \"\"\"\n",
    "        add a context. it should be a dictionary with the following structure:\n",
    "        {\n",
    "            'question_idA': ['contextA1', 'contextA2', ...],\n",
    "            'question_idB': ['contextB1', 'contextB2', ...],\n",
    "            ...\n",
    "        }\n",
    "\n",
    "        args:\n",
    "            context (dict): the context to add.\n",
    "            name (str): the name of the context.\n",
    "        \"\"\"\n",
    "        if not isinstance(context, dict):\n",
    "            raise TypeError('context must be a dictionary!')\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError('name must be a string!')\n",
    "        if name in self.contexts:\n",
    "            raise ValueError('context already exists!')\n",
    "        if not all(isinstance(k, str) and isinstance(v, list) for k, v in context.items()):\n",
    "            raise TypeError('invalid format!')\n",
    "\n",
    "        self.contexts[name] = context\n",
    "\n",
    "    def write_csv(self, batch_file: str, target: str, responses: list):\n",
    "        \"\"\"\n",
    "        Write responses to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            path_to_batch (str): The file path to the batch JSONL file.\n",
    "            responses (list): List of response dictionaries from the batch request.\n",
    "            csv_path (str): The file path to save the CSV file.\n",
    "        \"\"\"\n",
    "        with open(batch_file, 'r', encoding='utf-8') as batch:\n",
    "            batch_prompts = {}\n",
    "            for line in batch:\n",
    "                request = json.loads(line)\n",
    "                q_id = request['custom_id']\n",
    "                prompt = request['body']['messages'][-1]['content']\n",
    "                batch_prompts[q_id] = prompt\n",
    "\n",
    "        with open(target, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"Question ID\", \"GPT Response\", \"Ground Truth Answer\", \"Prompt\"])\n",
    "\n",
    "            for response in responses:\n",
    "                q_id = response['custom_id']\n",
    "                gpt_response = response['response']['body']['choices'][0]['message']['content']\n",
    "                question = self.dev[q_id]\n",
    "                ground_truth_answer = question.answer\n",
    "                prompt = batch_prompts[q_id]\n",
    "\n",
    "                writer.writerow([q_id, gpt_response, ground_truth_answer, prompt])\n",
    "\n",
    "    def _generate_date_string(self):\n",
    "        \"\"\"\n",
    "        generatw a date string in the format YYYYMMDD_HHMMSS.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated date string.\n",
    "        \"\"\"\n",
    "        date_string = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return date_string\n",
    "    \n",
    "    def _validate_num_questions(self, num_questions: int, context_name: str) -> int:\n",
    "        \"\"\"\n",
    "        Validate the number of questions to be processed.\n",
    "\n",
    "        Args:\n",
    "            num_questions (int): The number of questions to ask. If 0, all questions are processed.\n",
    "            context_name (str): The name of the context.\n",
    "\n",
    "        Returns:\n",
    "            int: The validated number of questions to process.\n",
    "        \"\"\"\n",
    "        total_questions = len(self.contexts[context_name])\n",
    "        if num_questions <= 0 or num_questions > total_questions:\n",
    "            num_questions = total_questions\n",
    "            print('Using all available questions.')\n",
    "        return num_questions\n",
    "\n",
    "    def _combine_contexts(self, contexts1: list[str], contexts2: list[str], position: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Combine two lists of contexts based on the specified position.\n",
    "\n",
    "        Args:\n",
    "            contexts1 (list[str]): The first list of contexts.\n",
    "            contexts2 (list[str]): The second list of contexts.\n",
    "            position (str): The position to place contexts from contexts1 in relation to contexts2.\n",
    "                            Can be \"first\", \"middle\", or \"last\".\n",
    "\n",
    "        Returns:\n",
    "            list[str]: The combined list of contexts.\n",
    "        \"\"\"\n",
    "        if position == \"first\":\n",
    "            return contexts1 + contexts2\n",
    "        elif position == \"middle\":\n",
    "            half = len(contexts2) // 2\n",
    "            return contexts2[:half] + contexts1 + contexts2[half:]\n",
    "        elif position == \"last\":\n",
    "            return contexts2 + contexts1\n",
    "        else:\n",
    "            raise ValueError('Invalid position value. It should be \"first\", \"middle\", or \"last\".')\n",
    "\n",
    "    def _build_request_dict(self, q_id: str, prompt: str) -> dict:\n",
    "        \"\"\"\n",
    "        Build a request dictionary for the OpenAI API.\n",
    "\n",
    "        Args:\n",
    "            q_id (str): The question ID.\n",
    "            prompt (str): The prompt to be used in the API request.\n",
    "\n",
    "        Returns:\n",
    "            dict: The request dictionary.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"custom_id\": q_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": self.model, \n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": 60\n",
    "                }\n",
    "            }   \n",
    "\n",
    "    def _create_prompt(self, contexts:list[str], question:str) -> str:\n",
    "        \"\"\"\n",
    "        create a prompt from the contexts and question.\n",
    "\n",
    "        args:\n",
    "            contexts (list[str]): the contexts\n",
    "            question (str): the questionn\n",
    "        \"\"\"\n",
    "        prompt = ''\n",
    "        for context in contexts:\n",
    "            prompt += context + '\\n\\n'\n",
    "        prompt += question\n",
    "        return prompt\n",
    "\n",
    "    def _adjust_k(self, k: int, contexts: list[str], k_adjustments: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Adjust the value of k based on the length of the contexts list.\n",
    "\n",
    "        Args:\n",
    "            k (int): The desired number of contexts to include.\n",
    "            contexts (list[str]): The list of contexts available.\n",
    "            k_adjustments (int): The current count of k adjustments.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The adjusted value of k and the updated count of k adjustments.\n",
    "        \"\"\"\n",
    "        if k == 0 or k > len(contexts):\n",
    "            k_adjustments += 1\n",
    "            k = len(contexts)\n",
    "        return k, k_adjustments\n",
    "    \n",
    "    def _print_k_adjustments_warning(self, k_adjustments: int, total: int, k_label: str = 'k') -> None:\n",
    "        \"\"\"\n",
    "        Print a warning if there were adjustments made to the value of k.\n",
    "\n",
    "        Args:\n",
    "            k_adjustments (int): The number of adjustments made to k.\n",
    "            num_batches (int): The number of batches processed.\n",
    "            k_label (str, optional): The label for k. Defaults to 'k'.\n",
    "        \"\"\"\n",
    "        if k_adjustments:\n",
    "            print(f\"WARNING: total adjustments of '{k_label}': {k_adjustments} of {total}\")\n",
    "            print(f\"This warning is because the specified {k_label} is outside the valid range or exceeds available contexts.\")\n",
    "\n",
    "    def _write_prompts_to_file(self, ids: list[str], prompts: list[str], path: str, file_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Write prompts to a text file.\n",
    "\n",
    "        Args:\n",
    "            prompts (list[str]): The prompts to write.\n",
    "            file_path (str): The file path to write to.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(f'{path}/{file_name}.txt', 'w') as file:\n",
    "            for i, prompt in enumerate(prompts):\n",
    "                file.write(f\"Question ID: {ids[i]}\\n\")\n",
    "                file.write(f\"{prompt}\\n\\n\")\n",
    "                file.write('--------------------------------------\\n\\n')\n",
    "\n",
    "    def _save_to_jsonl(self, batch: dict, path: str, file_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the batch requests to a JSONL file.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): The batch of requests to save.\n",
    "            path (str): The file path to save the JSONL file.\n",
    "        \"\"\"\n",
    "        with open(f'{path}/{file_name}.jsonl', 'w', encoding='utf-8') as f:\n",
    "            for request in batch.values():\n",
    "                f.write(json.dumps(request) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.write_csv(location, d, f'test_output/{output_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = load_json('contexts/results_10.json')\n",
    "\n",
    "def remove_duplicates(data):\n",
    "    for key, value in data.items():\n",
    "        unique_entries = []\n",
    "        seen_entries = set()\n",
    "        for entry in value:\n",
    "            cleaned_entry = ''.join(entry.split()).lower()\n",
    "            if cleaned_entry not in seen_entries:\n",
    "                unique_entries.append(entry)\n",
    "                seen_entries.add(cleaned_entry)\n",
    "        data[key] = unique_entries\n",
    "    return data\n",
    "\n",
    "keys = list(top10.keys())\n",
    "\n",
    "top10 = remove_duplicates(top10)\n",
    "\n",
    "for key, value in top10.items():\n",
    "    for i, _ in enumerate(value):\n",
    "        top10[key][i] = value[i].replace('\\n\\n', '\\n')\n",
    "\n",
    "save_json('contexts/top10_no_duplicates.json', top10)    \n",
    "\n",
    "_dev = load_json('dataset/_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = {}\n",
    "\n",
    "for question in _dev:  # iterate over each question in the full dataset\n",
    "    question_id = question['_id'] # get the id of the question\n",
    "    if question_id not in keys: continue  # skip if the question is not part of the ones from top10.json\n",
    "\n",
    "    full_context = question['context']  # get all contexts of the question\n",
    "    supporting_facts = question['supporting_facts']  # get the supporting facts of the question\n",
    "\n",
    "    context_list = []  # initialize an empty list to store context strings\n",
    "    oracle[question_id] = []  # initialize an empty list for the current question id in gold_context dict\n",
    "\n",
    "    for idx_context, context in enumerate(full_context):  # iterate over each given context in the dev dataset\n",
    "        title = context[0]  # get the title of the context\n",
    "        if title not in [sf[0] for sf in supporting_facts]: continue  # skip if the title is not in the supporting facts\n",
    "        # because we only want to include the context if it is a supporting fact\n",
    "\n",
    "        string = title + '\\n' # start the context string with the title and a newline\n",
    "        for i, c in enumerate(context[1]): # iterate over each sentence in the context\n",
    "            string += c  # append the sentence to the context string\n",
    "\n",
    "        oracle[question_id].append(string) # add the context string to the list for the currennt question id\n",
    "\n",
    "save_json('contexts/oracle.json', oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negatives = {}\n",
    "\n",
    "for question in _dev: # iterate over each question in the _dev dataset\n",
    "    question_id = question['_id'] # get the id of the question\n",
    "    if question_id not in keys: continue # skip if the question is not part of the ones from top10.json\n",
    "    \n",
    "    hard_negatives[question_id] = []  # initialize an empty list for the current question id in hard_negatives\n",
    "    supporting_facts = question['supporting_facts'] # get the supporting facts of the question\n",
    "\n",
    "    titles_top10 = [t.split('\\n')[0] for t in top10[question_id]] # get the titles from the top10 for the current question id\n",
    "\n",
    "    for idx_title, title in enumerate(titles_top10): # iterate over each title in the top10 titles\n",
    "        if title not in [sf[0] for sf in supporting_facts]: # if the title is not in the supporting facts\n",
    "            hard_negatives[question_id].append(top10[question_id][idx_title]) # we only want to include the context if it is not a\n",
    "            # supporting fact but seems useful according to the retriever\n",
    "\n",
    "save_json('contexts/hard_negatives.json', hard_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563424\n"
     ]
    }
   ],
   "source": [
    "randomly_drawn = {}\n",
    "complete_corpus = list(load_json('dataset/wiki_musique_corpus.json').values())\n",
    "\n",
    "print(len(complete_corpus))\n",
    "\n",
    "num_contexts = 10\n",
    "\n",
    "for key in keys:\n",
    "    randomly_drawn[key] = []\n",
    "    for _ in range(10):\n",
    "        choice = random.choice(complete_corpus)\n",
    "        title = choice['title']\n",
    "        text = choice['text']\n",
    "        randomly_drawn[key].append(title + '\\n' + text)\n",
    "\n",
    "save_json('contexts/random.json', randomly_drawn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negatives = load_json('contexts/hard_negatives.json')\n",
    "oracle = load_json('contexts/oracle.json')\n",
    "randomly_drawn = load_json('contexts/random.json')\n",
    "top10 = load_json('contexts/top10_no_duplicates.json')\n",
    "gibberish = load_json('contexts/gibberish.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT:\n",
      "Answer in a concise way. NO full sentences! As few words as possible! For example just a date, the name of a place, the name of a person, a number, a yes or no, etc.\n",
      "\n",
      "CONTEXTS:\n",
      "hard_negatives\n",
      "oracle\n",
      "random\n",
      "top10\n",
      "gibberish\n"
     ]
    }
   ],
   "source": [
    "system_prompt = 'Answer in a concise way. NO full sentences! As few words as possible! For example just a date, the name of a place, the name of a person, a number, a yes or no, etc.'\n",
    "\n",
    "qa = QA('dataset/_dev.json', model='gpt-3.5-turbo', system_prompt=system_prompt, actually_prompt=False)\n",
    "\n",
    "qa.add_context(hard_negatives, 'hard_negatives')\n",
    "qa.add_context(oracle, 'oracle')\n",
    "qa.add_context(randomly_drawn, 'random')\n",
    "qa.add_context(top10, 'top10')\n",
    "qa.add_context(gibberish, 'gibberish')\n",
    "\n",
    "# print system prompt\n",
    "print('SYSTEM PROMPT:')\n",
    "print(qa.system_prompt)\n",
    "\n",
    "# print context names\n",
    "print('\\nCONTEXTS:')\n",
    "qa.print_context_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all available questions.\n",
      "Creating batch for 1199 questions from 'top10' and 'random' contexts...\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_1.jsonl' submitted with ID: batch_cyvP66p47P5rgC0sdIDzAm2x\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_2.jsonl' submitted with ID: batch_esT6brKFQqz0vB5JdU4WAoCs\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_3.jsonl' submitted with ID: batch_JEVpiyEiW4MS9gDcy1rwDqTb\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_4.jsonl' submitted with ID: batch_no9OEGl4fNNByXLbEKuaoOt7\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_5.jsonl' submitted with ID: batch_EqCpVnyTVWREwAeGprzgt54g\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_6.jsonl' submitted with ID: batch_aFNwTlkGtcPWJI7YpaEUEmaO\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_7.jsonl' submitted with ID: batch_xGF9Ci0A55FUZeSJCiQfP48j\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_8.jsonl' submitted with ID: batch_mUzIZmgTOJhMTq2Hc7EhV01s\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_9.jsonl' submitted with ID: batch_xBOzR9e1EuOxhON3qGIRirrK\n",
      "Batch job 'results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_10.jsonl' submitted with ID: batch_Y8XjmtNPEtBpaQwnycsGZeVW\n",
      "Using all available questions.\n",
      "Creating batch for 1199 questions from 'top10' and 'random' contexts...\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_1.jsonl' submitted with ID: batch_3SpLbigKfPl1sQzQ2f5Gg6r9\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_2.jsonl' submitted with ID: batch_TOFE71vbDNFG2jUuFnRhc1QM\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_3.jsonl' submitted with ID: batch_6yBQ5WVQygLo0KT8mvchL1xm\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_4.jsonl' submitted with ID: batch_0qHbAaadZu41OKesqF9p7jab\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_5.jsonl' submitted with ID: batch_T9Zj6uS3X3TvK3VhCZbpWwZP\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_6.jsonl' submitted with ID: batch_xQ0FNPQvJ4dVhGJKRcZHKyt4\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_7.jsonl' submitted with ID: batch_Drat6sDbcelZr40KXtPuQH28\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_8.jsonl' submitted with ID: batch_S2nwvjLDnj9Ts4AQPYTCWlMJ\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_9.jsonl' submitted with ID: batch_PCO8hnKkRtKsxvWMGNPO6vH5\n",
      "Batch job 'results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_10.jsonl' submitted with ID: batch_3w0vf7sIV3tWnGUb1WmxUFRD\n",
      "Using all available questions.\n",
      "Creating batch for 1199 questions from 'top10' and 'random' contexts...\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_1.jsonl' submitted with ID: batch_LYq43Ig5GH1412nDi42umIki\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_2.jsonl' submitted with ID: batch_DgWFrZzJplO9rKb3kHrJvJHK\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_3.jsonl' submitted with ID: batch_pCfxoJBrK8ejOYgZxMG9SA3Z\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_4.jsonl' submitted with ID: batch_EiBdmjXkGbaXyJ1w9mZ0RzOS\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_5.jsonl' submitted with ID: batch_WJ8OfmAOH3XYoPTYZZNnMxAF\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_6.jsonl' submitted with ID: batch_9fABac9kT0ThJFMn1kFuqx9h\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_7.jsonl' submitted with ID: batch_OWZEi4CP5hkuHkcsg5vmc3lk\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_8.jsonl' submitted with ID: batch_oMRHk4BrNd6mqjORvFMCAB4B\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_9.jsonl' submitted with ID: batch_kBwrFDmrAdx2I5yAkk0XD6sJ\n",
      "Batch job 'results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_10.jsonl' submitted with ID: batch_mtJjGFxK8FyfvaWN3SYqTubo\n",
      "Using all available questions.\n",
      "Creating batch for 1199 questions from 'top10' and 'random' contexts...\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_1.jsonl' submitted with ID: batch_3DFjOCtG9qrTOtUQnCO3ZRTL\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_2.jsonl' submitted with ID: batch_jK0BTM318oZEjLvrhfJqyzOR\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_3.jsonl' submitted with ID: batch_SERXG7lDG8oXnbdpG2gxx7yS\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_4.jsonl' submitted with ID: batch_GS9kxs0YYam3IZUIaic9r1K0\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_5.jsonl' submitted with ID: batch_cFyCuz1Sn9BBM51aozkjKs3S\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_6.jsonl' submitted with ID: batch_RGJ1CRlrQgPEOruSwFBS6LKj\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_7.jsonl' submitted with ID: batch_IyVFScvAzxvNN53IJ0nb5dvy\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_8.jsonl' submitted with ID: batch_9EilhlA6CUNe2posFl8VeIki\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_9.jsonl' submitted with ID: batch_ai6cCsfJUwbDeYTeXnkNhrar\n",
      "Batch job 'results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_10.jsonl' submitted with ID: batch_aFqo3F2HaTCLvE6iGi41Mx2c\n",
      "Using all available questions.\n",
      "Creating batch for 1199 questions from 'top10' and 'random' contexts...\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_1.jsonl' submitted with ID: batch_gtnwXuZNFXfMIEAopr3RfHMr\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_2.jsonl' submitted with ID: batch_XyuSeGtGQ5Z3ZycKuQDJ6ctO\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_3.jsonl' submitted with ID: batch_cCypYk0A7GbMFnPkvAvKyVtD\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_4.jsonl' submitted with ID: batch_xWOeJ3O1gghXiGh6HPWpgSqC\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_5.jsonl' submitted with ID: batch_J682ysYpyHbYfAvcYwjgr5gF\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_6.jsonl' submitted with ID: batch_L7xkGBKskr6jyOroznMl5G99\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_7.jsonl' submitted with ID: batch_I1WSgmo02NNncEtvHuYa70zv\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_8.jsonl' submitted with ID: batch_p9C9n5BTduu0gBcqZ4Xhvwy3\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_9.jsonl' submitted with ID: batch_FCbioRELRbz6ZX25nVutHx74\n",
      "Batch job 'results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_10.jsonl' submitted with ID: batch_2S8DVXtRCUGQTOnXgQeCIbru\n",
      "Using all available questions.\n",
      "Creating batch for 1199 questions from 'top10' and 'random' contexts...\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_1.jsonl' submitted with ID: batch_brJ0gPsMsW80HvyJfs6PJR5J\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_2.jsonl' submitted with ID: batch_2ACXVjAgVHsvA8Dh1VxvX0Oe\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_3.jsonl' submitted with ID: batch_sJ3W69yyTJZWN0qnAW7fTG6A\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_4.jsonl' submitted with ID: batch_fELYuRiaT6D0xUMGiPvP9XTs\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_5.jsonl' submitted with ID: batch_eO90bfA8Z9NmWX7daTlVfo2k\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_6.jsonl' submitted with ID: batch_3jgoKa16sDk4mZ21IZiAKqKI\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_7.jsonl' submitted with ID: batch_45zFzvQjK0kGfQ6wy5pZKFQQ\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_8.jsonl' submitted with ID: batch_6JgCT9eqYwJ7ubXJOKlaZ7qb\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_9.jsonl' submitted with ID: batch_xf8I4jon7Oamgm2oDE98suOS\n",
      "Batch job 'results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_10.jsonl' submitted with ID: batch_Hipv4hTjrOJM7UexJOcFPwYg\n",
      "Using all available questions.\n",
      "Creating batch for 1199 questions from 'top10' and 'random' contexts...\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_1.jsonl' submitted with ID: batch_ip1wIDYilNyL4xAnVMpXRESc\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_2.jsonl' submitted with ID: batch_XcLjAxtNWTTVTpTqgYPJkW23\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_3.jsonl' submitted with ID: batch_UJgaTn878FtdegYIBIQcTdR2\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_4.jsonl' submitted with ID: batch_EUr7ZxtIiUDxDZnINjCkt3cS\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_5.jsonl' submitted with ID: batch_eEmDXzLhAUx4qXhY5MhEtFjh\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_6.jsonl' submitted with ID: batch_G0MYaq9Zev7IcvwXR8zccCN0\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_7.jsonl' submitted with ID: batch_iQLkKxW6JwnD1RmBbO83E64d\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_8.jsonl' submitted with ID: batch_0SODfFQTCb1nT2YbDU1XJdft\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_9.jsonl' submitted with ID: batch_PGtdmEtIidELkAhvjQmo3nt4\n",
      "Batch job 'results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_10.jsonl' submitted with ID: batch_7VFQUpiTGSV9ZAW2TqV35AGt\n",
      "Using all available questions.\n",
      "Creating batch for 1199 questions from 'top10' and 'random' contexts...\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_1.jsonl' submitted with ID: batch_YNLdSibh0SozIWc4Ivlufl7G\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_2.jsonl' submitted with ID: batch_aDOqtjBlJwCZC1V7wcROTpEi\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_3.jsonl' submitted with ID: batch_PLs370rS9RUuXxFZbWOlcNwu\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_4.jsonl' submitted with ID: batch_q5kGgHx8ytHwt6IHaULr9qaC\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_5.jsonl' submitted with ID: batch_DQuRG7sgq2bpYo3KjlaGr3sD\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_6.jsonl' submitted with ID: batch_yh7rdRS5GwwlSO3hK77AiK2K\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_7.jsonl' submitted with ID: batch_hSr6ppz0aFrVDaCNCwpc8Fx7\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_8.jsonl' submitted with ID: batch_N2N19Hwdn1AoHGQ4eI1gKVlz\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_9.jsonl' submitted with ID: batch_Msw6NMfVL8w1z2Vjs0Tq9qtp\n",
      "Batch job 'results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_10.jsonl' submitted with ID: batch_SogqZD88O83XpKQRainhk0wV\n",
      "Using all available questions.\n",
      "Creating batch for 1199 questions from 'top10' and 'random' contexts...\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_1.jsonl' submitted with ID: batch_h3YMd5h3TBSclkvgRfzRMYIi\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_2.jsonl' submitted with ID: batch_4DpmEn2RMEorXgp4jIgFMkXG\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_3.jsonl' submitted with ID: batch_E5OAdbliOubJfrNRV0fYDleb\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_4.jsonl' submitted with ID: batch_LYmkL5FurN1k2aWrCMCTcrPc\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_5.jsonl' submitted with ID: batch_RaPzCwFqCYuAIiTnVS0rfUon\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_6.jsonl' submitted with ID: batch_GIjAGnXeMvJhuPQeNK2NKn4y\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_7.jsonl' submitted with ID: batch_Sm1VPYAWpjbAaXXGwB7w8rWF\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_8.jsonl' submitted with ID: batch_6risOVAQWJAOOOJQvQgVuRav\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_9.jsonl' submitted with ID: batch_iC9AhAhrJ1AeZWMxVngB0Zdo\n",
      "Batch job 'results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_10.jsonl' submitted with ID: batch_04Othwadlu4mCd3wuwSiMSo2\n"
     ]
    }
   ],
   "source": [
    "actually_prompt = True\n",
    "num_questions = 0\n",
    "description = 'random injected noise, serious run 1'\n",
    "\n",
    "K_top10 = [1, 3, 5]\n",
    "positions = ['first', 'middle', 'last']\n",
    "\n",
    "current_ids = []\n",
    "\n",
    "for k in K_top10:\n",
    "    for position in positions:\n",
    "        location = f'results/injected_noise/{k}_{position}'\n",
    "        batch_path = qa.batch_by_structured_mixed_context('top10', 'random', 'results', num_questions=num_questions, k1=k, k2=2, position=position, write_debug_file=True)\n",
    "        if actually_prompt:\n",
    "            batch_job_ids = qa.submit_split_batch(batch_path, f'{description}, k={k}, pos={position}', 10)\n",
    "            current_ids.extend(batch_job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job results for batch_cyvP66p47P5rgC0sdIDzAm2x - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_1.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_1_rslt.jsonl\n",
      "batch job results for batch_esT6brKFQqz0vB5JdU4WAoCs - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_2.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_2_rslt.jsonl\n",
      "batch job results for batch_JEVpiyEiW4MS9gDcy1rwDqTb - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_3.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_3_rslt.jsonl\n",
      "batch job results for batch_no9OEGl4fNNByXLbEKuaoOt7 - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_4.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_4_rslt.jsonl\n",
      "batch job results for batch_EqCpVnyTVWREwAeGprzgt54g - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_5.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_5_rslt.jsonl\n",
      "batch job results for batch_aFNwTlkGtcPWJI7YpaEUEmaO - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_6.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_6_rslt.jsonl\n",
      "batch job results for batch_xGF9Ci0A55FUZeSJCiQfP48j - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_7.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_7_rslt.jsonl\n",
      "batch job results for batch_mUzIZmgTOJhMTq2Hc7EhV01s - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_8.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_8_rslt.jsonl\n",
      "batch job results for batch_xBOzR9e1EuOxhON3qGIRirrK - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_9.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_9_rslt.jsonl\n",
      "batch job results for batch_Y8XjmtNPEtBpaQwnycsGZeVW - results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_10.jsonl saved to results/top10-random-k1_1-k2_2_first/20240613_024222/submitted_batch_part_10_rslt.jsonl\n",
      "batch job results for batch_3SpLbigKfPl1sQzQ2f5Gg6r9 - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_1.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_1_rslt.jsonl\n",
      "batch job results for batch_TOFE71vbDNFG2jUuFnRhc1QM - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_2.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_2_rslt.jsonl\n",
      "batch job results for batch_6yBQ5WVQygLo0KT8mvchL1xm - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_3.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_3_rslt.jsonl\n",
      "batch job results for batch_0qHbAaadZu41OKesqF9p7jab - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_4.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_4_rslt.jsonl\n",
      "batch job results for batch_T9Zj6uS3X3TvK3VhCZbpWwZP - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_5.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_5_rslt.jsonl\n",
      "batch job results for batch_xQ0FNPQvJ4dVhGJKRcZHKyt4 - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_6.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_6_rslt.jsonl\n",
      "batch job results for batch_Drat6sDbcelZr40KXtPuQH28 - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_7.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_7_rslt.jsonl\n",
      "batch job results for batch_S2nwvjLDnj9Ts4AQPYTCWlMJ - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_8.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_8_rslt.jsonl\n",
      "batch job results for batch_PCO8hnKkRtKsxvWMGNPO6vH5 - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_9.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_9_rslt.jsonl\n",
      "batch job results for batch_3w0vf7sIV3tWnGUb1WmxUFRD - results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_10.jsonl saved to results/top10-random-k1_1-k2_2_middle/20240613_030401/submitted_batch_part_10_rslt.jsonl\n",
      "batch job results for batch_LYq43Ig5GH1412nDi42umIki - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_1.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_1_rslt.jsonl\n",
      "batch job results for batch_DgWFrZzJplO9rKb3kHrJvJHK - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_2.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_2_rslt.jsonl\n",
      "batch job results for batch_pCfxoJBrK8ejOYgZxMG9SA3Z - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_3.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_3_rslt.jsonl\n",
      "batch job results for batch_EiBdmjXkGbaXyJ1w9mZ0RzOS - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_4.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_4_rslt.jsonl\n",
      "batch job results for batch_WJ8OfmAOH3XYoPTYZZNnMxAF - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_5.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_5_rslt.jsonl\n",
      "batch job results for batch_9fABac9kT0ThJFMn1kFuqx9h - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_6.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_6_rslt.jsonl\n",
      "batch job results for batch_OWZEi4CP5hkuHkcsg5vmc3lk - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_7.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_7_rslt.jsonl\n",
      "batch job results for batch_oMRHk4BrNd6mqjORvFMCAB4B - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_8.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_8_rslt.jsonl\n",
      "batch job results for batch_kBwrFDmrAdx2I5yAkk0XD6sJ - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_9.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_9_rslt.jsonl\n",
      "batch job results for batch_mtJjGFxK8FyfvaWN3SYqTubo - results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_10.jsonl saved to results/top10-random-k1_1-k2_2_last/20240613_032308/submitted_batch_part_10_rslt.jsonl\n",
      "batch job results for batch_3DFjOCtG9qrTOtUQnCO3ZRTL - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_1.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_1_rslt.jsonl\n",
      "batch job results for batch_jK0BTM318oZEjLvrhfJqyzOR - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_2.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_2_rslt.jsonl\n",
      "batch job results for batch_SERXG7lDG8oXnbdpG2gxx7yS - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_3.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_3_rslt.jsonl\n",
      "batch job results for batch_GS9kxs0YYam3IZUIaic9r1K0 - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_4.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_4_rslt.jsonl\n",
      "batch job results for batch_cFyCuz1Sn9BBM51aozkjKs3S - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_5.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_5_rslt.jsonl\n",
      "batch job results for batch_RGJ1CRlrQgPEOruSwFBS6LKj - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_6.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_6_rslt.jsonl\n",
      "batch job results for batch_IyVFScvAzxvNN53IJ0nb5dvy - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_7.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_7_rslt.jsonl\n",
      "batch job results for batch_9EilhlA6CUNe2posFl8VeIki - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_8.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_8_rslt.jsonl\n",
      "batch job results for batch_ai6cCsfJUwbDeYTeXnkNhrar - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_9.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_9_rslt.jsonl\n",
      "batch job results for batch_aFqo3F2HaTCLvE6iGi41Mx2c - results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_10.jsonl saved to results/top10-random-k1_3-k2_2_first/20240613_033239/submitted_batch_part_10_rslt.jsonl\n",
      "batch job results for batch_gtnwXuZNFXfMIEAopr3RfHMr - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_1.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_1_rslt.jsonl\n",
      "batch job results for batch_XyuSeGtGQ5Z3ZycKuQDJ6ctO - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_2.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_2_rslt.jsonl\n",
      "batch job results for batch_cCypYk0A7GbMFnPkvAvKyVtD - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_3.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_3_rslt.jsonl\n",
      "batch job results for batch_xWOeJ3O1gghXiGh6HPWpgSqC - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_4.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_4_rslt.jsonl\n",
      "batch job results for batch_J682ysYpyHbYfAvcYwjgr5gF - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_5.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_5_rslt.jsonl\n",
      "batch job results for batch_L7xkGBKskr6jyOroznMl5G99 - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_6.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_6_rslt.jsonl\n",
      "batch job results for batch_I1WSgmo02NNncEtvHuYa70zv - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_7.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_7_rslt.jsonl\n",
      "batch job results for batch_p9C9n5BTduu0gBcqZ4Xhvwy3 - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_8.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_8_rslt.jsonl\n",
      "batch job results for batch_FCbioRELRbz6ZX25nVutHx74 - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_9.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_9_rslt.jsonl\n",
      "batch job results for batch_2S8DVXtRCUGQTOnXgQeCIbru - results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_10.jsonl saved to results/top10-random-k1_3-k2_2_middle/20240613_034111/submitted_batch_part_10_rslt.jsonl\n",
      "batch job results for batch_brJ0gPsMsW80HvyJfs6PJR5J - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_1.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_1_rslt.jsonl\n",
      "batch job results for batch_2ACXVjAgVHsvA8Dh1VxvX0Oe - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_2.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_2_rslt.jsonl\n",
      "batch job results for batch_sJ3W69yyTJZWN0qnAW7fTG6A - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_3.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_3_rslt.jsonl\n",
      "batch job results for batch_fELYuRiaT6D0xUMGiPvP9XTs - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_4.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_4_rslt.jsonl\n",
      "batch job results for batch_eO90bfA8Z9NmWX7daTlVfo2k - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_5.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_5_rslt.jsonl\n",
      "batch job results for batch_3jgoKa16sDk4mZ21IZiAKqKI - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_6.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_6_rslt.jsonl\n",
      "batch job results for batch_45zFzvQjK0kGfQ6wy5pZKFQQ - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_7.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_7_rslt.jsonl\n",
      "batch job results for batch_6JgCT9eqYwJ7ubXJOKlaZ7qb - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_8.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_8_rslt.jsonl\n",
      "batch job results for batch_xf8I4jon7Oamgm2oDE98suOS - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_9.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_9_rslt.jsonl\n",
      "batch job results for batch_Hipv4hTjrOJM7UexJOcFPwYg - results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_10.jsonl saved to results/top10-random-k1_3-k2_2_last/20240613_034840/submitted_batch_part_10_rslt.jsonl\n",
      "batch job results for batch_ip1wIDYilNyL4xAnVMpXRESc - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_1.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_1_rslt.jsonl\n",
      "batch job results for batch_XcLjAxtNWTTVTpTqgYPJkW23 - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_2.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_2_rslt.jsonl\n",
      "batch job results for batch_UJgaTn878FtdegYIBIQcTdR2 - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_3.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_3_rslt.jsonl\n",
      "batch job results for batch_EUr7ZxtIiUDxDZnINjCkt3cS - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_4.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_4_rslt.jsonl\n",
      "batch job results for batch_eEmDXzLhAUx4qXhY5MhEtFjh - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_5.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_5_rslt.jsonl\n",
      "batch job results for batch_G0MYaq9Zev7IcvwXR8zccCN0 - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_6.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_6_rslt.jsonl\n",
      "batch job results for batch_iQLkKxW6JwnD1RmBbO83E64d - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_7.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_7_rslt.jsonl\n",
      "batch job results for batch_0SODfFQTCb1nT2YbDU1XJdft - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_8.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_8_rslt.jsonl\n",
      "batch job results for batch_PGtdmEtIidELkAhvjQmo3nt4 - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_9.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_9_rslt.jsonl\n",
      "batch job results for batch_7VFQUpiTGSV9ZAW2TqV35AGt - results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_10.jsonl saved to results/top10-random-k1_5-k2_2_first/20240613_035407/submitted_batch_part_10_rslt.jsonl\n",
      "batch job results for batch_YNLdSibh0SozIWc4Ivlufl7G - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_1.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_1_rslt.jsonl\n",
      "batch job results for batch_aDOqtjBlJwCZC1V7wcROTpEi - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_2.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_2_rslt.jsonl\n",
      "batch job results for batch_PLs370rS9RUuXxFZbWOlcNwu - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_3.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_3_rslt.jsonl\n",
      "batch job results for batch_q5kGgHx8ytHwt6IHaULr9qaC - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_4.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_4_rslt.jsonl\n",
      "batch job results for batch_DQuRG7sgq2bpYo3KjlaGr3sD - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_5.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_5_rslt.jsonl\n",
      "batch job results for batch_yh7rdRS5GwwlSO3hK77AiK2K - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_6.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_6_rslt.jsonl\n",
      "batch job results for batch_hSr6ppz0aFrVDaCNCwpc8Fx7 - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_7.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_7_rslt.jsonl\n",
      "batch job results for batch_N2N19Hwdn1AoHGQ4eI1gKVlz - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_8.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_8_rslt.jsonl\n",
      "batch job results for batch_Msw6NMfVL8w1z2Vjs0Tq9qtp - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_9.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_9_rslt.jsonl\n",
      "batch job results for batch_SogqZD88O83XpKQRainhk0wV - results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_10.jsonl saved to results/top10-random-k1_5-k2_2_middle/20240613_040444/submitted_batch_part_10_rslt.jsonl\n",
      "batch job results for batch_h3YMd5h3TBSclkvgRfzRMYIi - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_1.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_1_rslt.jsonl\n",
      "batch job results for batch_4DpmEn2RMEorXgp4jIgFMkXG - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_2.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_2_rslt.jsonl\n",
      "batch job results for batch_E5OAdbliOubJfrNRV0fYDleb - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_3.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_3_rslt.jsonl\n",
      "batch job results for batch_LYmkL5FurN1k2aWrCMCTcrPc - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_4.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_4_rslt.jsonl\n",
      "batch job results for batch_RaPzCwFqCYuAIiTnVS0rfUon - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_5.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_5_rslt.jsonl\n",
      "batch job results for batch_GIjAGnXeMvJhuPQeNK2NKn4y - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_6.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_6_rslt.jsonl\n",
      "batch job results for batch_Sm1VPYAWpjbAaXXGwB7w8rWF - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_7.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_7_rslt.jsonl\n",
      "batch job results for batch_6risOVAQWJAOOOJQvQgVuRav - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_8.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_8_rslt.jsonl\n",
      "batch job results for batch_iC9AhAhrJ1AeZWMxVngB0Zdo - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_9.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_9_rslt.jsonl\n",
      "batch job results for batch_04Othwadlu4mCd3wuwSiMSo2 - results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_10.jsonl saved to results/top10-random-k1_5-k2_2_last/20240613_042557/submitted_batch_part_10_rslt.jsonl\n"
     ]
    }
   ],
   "source": [
    "result = qa.check_batches(current_ids, 'batches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['submitted_batch_part_1_rslt.csv', 'submitted_batch_part_2_rslt.csv', 'submitted_batch_part_3_rslt.csv', 'submitted_batch_part_4_rslt.csv', 'submitted_batch_part_5_rslt.csv', 'submitted_batch_part_6_rslt.csv', 'submitted_batch_part_7_rslt.csv', 'submitted_batch_part_8_rslt.csv', 'submitted_batch_part_9_rslt.csv', 'submitted_batch_part_10_rslt.csv']\n",
      "./results/top10-random-k1_3-k2_2_middle/20240613_034111/csv/results.csv: 1200 rows\n",
      "['submitted_batch_part_1_rslt.csv', 'submitted_batch_part_2_rslt.csv', 'submitted_batch_part_3_rslt.csv', 'submitted_batch_part_4_rslt.csv', 'submitted_batch_part_5_rslt.csv', 'submitted_batch_part_6_rslt.csv', 'submitted_batch_part_7_rslt.csv', 'submitted_batch_part_8_rslt.csv', 'submitted_batch_part_9_rslt.csv', 'submitted_batch_part_10_rslt.csv']\n",
      "./results/top10-random-k1_3-k2_2_last/20240613_034840/csv/results.csv: 1200 rows\n",
      "['submitted_batch_part_1_rslt.csv', 'submitted_batch_part_2_rslt.csv', 'submitted_batch_part_3_rslt.csv', 'submitted_batch_part_4_rslt.csv', 'submitted_batch_part_5_rslt.csv', 'submitted_batch_part_6_rslt.csv', 'submitted_batch_part_7_rslt.csv', 'submitted_batch_part_8_rslt.csv', 'submitted_batch_part_9_rslt.csv', 'submitted_batch_part_10_rslt.csv']\n",
      "./results/top10-random-k1_5-k2_2_middle/20240613_040444/csv/results.csv: 1200 rows\n",
      "['submitted_batch_part_1_rslt.csv', 'submitted_batch_part_2_rslt.csv', 'submitted_batch_part_3_rslt.csv', 'submitted_batch_part_4_rslt.csv', 'submitted_batch_part_5_rslt.csv', 'submitted_batch_part_6_rslt.csv', 'submitted_batch_part_7_rslt.csv', 'submitted_batch_part_8_rslt.csv', 'submitted_batch_part_9_rslt.csv', 'submitted_batch_part_10_rslt.csv']\n",
      "./results/top10-random-k1_5-k2_2_last/20240613_042557/csv/results.csv: 1200 rows\n",
      "['submitted_batch_part_1_rslt.csv', 'submitted_batch_part_2_rslt.csv', 'submitted_batch_part_3_rslt.csv', 'submitted_batch_part_4_rslt.csv', 'submitted_batch_part_5_rslt.csv', 'submitted_batch_part_6_rslt.csv', 'submitted_batch_part_7_rslt.csv', 'submitted_batch_part_8_rslt.csv', 'submitted_batch_part_9_rslt.csv', 'submitted_batch_part_10_rslt.csv']\n",
      "./results/top10-random-k1_5-k2_2_first/20240613_035407/csv/results.csv: 1200 rows\n",
      "['submitted_batch_part_1_rslt.csv', 'submitted_batch_part_2_rslt.csv', 'submitted_batch_part_3_rslt.csv', 'submitted_batch_part_4_rslt.csv', 'submitted_batch_part_5_rslt.csv', 'submitted_batch_part_6_rslt.csv', 'submitted_batch_part_7_rslt.csv', 'submitted_batch_part_8_rslt.csv', 'submitted_batch_part_9_rslt.csv', 'submitted_batch_part_10_rslt.csv']\n",
      "./results/top10-random-k1_3-k2_2_first/20240613_033239/csv/results.csv: 1200 rows\n",
      "['submitted_batch_part_1_rslt.csv', 'submitted_batch_part_2_rslt.csv', 'submitted_batch_part_3_rslt.csv', 'submitted_batch_part_4_rslt.csv', 'submitted_batch_part_5_rslt.csv', 'submitted_batch_part_6_rslt.csv', 'submitted_batch_part_7_rslt.csv', 'submitted_batch_part_8_rslt.csv', 'submitted_batch_part_9_rslt.csv', 'submitted_batch_part_10_rslt.csv']\n",
      "./results/top10-random-k1_1-k2_2_last/20240613_032308/csv/results.csv: 1200 rows\n",
      "['submitted_batch_part_1_rslt.csv', 'submitted_batch_part_2_rslt.csv', 'submitted_batch_part_3_rslt.csv', 'submitted_batch_part_4_rslt.csv', 'submitted_batch_part_5_rslt.csv', 'submitted_batch_part_6_rslt.csv', 'submitted_batch_part_7_rslt.csv', 'submitted_batch_part_8_rslt.csv', 'submitted_batch_part_9_rslt.csv', 'submitted_batch_part_10_rslt.csv']\n",
      "./results/top10-random-k1_1-k2_2_middle/20240613_030401/csv/results.csv: 1200 rows\n",
      "['submitted_batch_part_1_rslt.csv', 'submitted_batch_part_2_rslt.csv', 'submitted_batch_part_3_rslt.csv', 'submitted_batch_part_4_rslt.csv', 'submitted_batch_part_5_rslt.csv', 'submitted_batch_part_6_rslt.csv', 'submitted_batch_part_7_rslt.csv', 'submitted_batch_part_8_rslt.csv', 'submitted_batch_part_9_rslt.csv', 'submitted_batch_part_10_rslt.csv']\n",
      "./results/top10-random-k1_1-k2_2_first/20240613_024222/csv/results.csv: 1200 rows\n"
     ]
    }
   ],
   "source": [
    "concatenate_csv_files('./results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HARD NEGATIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch job status: completed (65s)         \n"
     ]
    }
   ],
   "source": [
    "qa.actually_prompt = True\n",
    "actually_prompt = True\n",
    "\n",
    "location = 'results/hard_negatives'\n",
    "\n",
    "K = [1, 3, 5]\n",
    "\n",
    "for k in K:\n",
    "    if k > 1: break\n",
    "    file_name = f'e4_qa_hard_negatives_k{k}'\n",
    "    batch_loc_name = qa.batch_by_context('hard_negatives', location, file_name, num_questions=1, k=k, write_debug_file=True)\n",
    "    if actually_prompt:\n",
    "        responses = qa.ask_batch(batch_loc_name[0], batch_loc_name[1], f'QA on hard negatives with k={k}')\n",
    "        qa.write_csv(batch_loc_name[0], batch_loc_name[1], responses)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
