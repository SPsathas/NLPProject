{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# import glob\n",
    "# import hashlib\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "# import uuid\n",
    "from typing import Any\n",
    "\n",
    "# import backoff\n",
    "import openai\n",
    "# from httpx import HTTPStatusError\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: str) -> Any:\n",
    "    \"\"\"load a json file from the specified path.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def save_json(path: str, data: Any) -> None:\n",
    "    \"\"\"save data to a json file at the specified path.\"\"\"\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def sortf(file_name):\n",
    "    \"\"\"\n",
    "    sort function for concatenate_csv_files() files.\n",
    "    extracts the part number from the file name.\n",
    "    very specific to the file naming convention used in the project.\n",
    "    \"\"\"\n",
    "    match = re.search(r'part_(\\d+)_rslt\\.csv', file_name)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "def concatenate_csv_files(root_dir):\n",
    "    \"\"\"\n",
    "    concatenate all csv files in the specified directory and its subdirectories.\n",
    "    the concatenated files are saved as 'results.csv' in the same directory as the csv files.\n",
    "    very specific to the file naming convention used in the project.\n",
    "    \"\"\"\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        csv_files = [f for f in files if f.endswith('.csv') and f != 'results.csv']\n",
    "        \n",
    "        if csv_files:\n",
    "            csv_files.sort(key=sortf)\n",
    "            print(csv_files)\n",
    "            results_file_path = os.path.join(subdir, 'results.csv')\n",
    "            \n",
    "            with open(results_file_path, 'w', newline='') as results_file:\n",
    "                writer = csv.writer(results_file)\n",
    "                header_written = False\n",
    "                \n",
    "                for file_name in csv_files:\n",
    "                    file_path = os.path.join(subdir, file_name)\n",
    "                    with open(file_path, 'r') as csv_file:\n",
    "                        reader = csv.reader(csv_file)\n",
    "                        header = next(reader)\n",
    "                        \n",
    "                        if not header_written:\n",
    "                            writer.writerow(header)\n",
    "                            header_written = True\n",
    "                        \n",
    "                        for row in reader:\n",
    "                            writer.writerow(row)\n",
    "                \n",
    "            with open(results_file_path, 'r') as results_file:\n",
    "                reader = csv.reader(results_file)\n",
    "                print(f'{results_file_path}: {sum(1 for _ in reader)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevQuestions:\n",
    "    class Question:\n",
    "        def __init__(self, identifier: str, question: str, answer: str):\n",
    "            \"\"\"\n",
    "            initialize a Question instance.\n",
    "\n",
    "            Args:\n",
    "                identifier (str): the identifier for the question.\n",
    "                question (str): the question text.\n",
    "                answer (str): the answer text.\n",
    "            \"\"\"\n",
    "            self.id = identifier\n",
    "            self.question = question\n",
    "            self.answer = answer\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"\n",
    "        initialize the DevQuestions instance by loading questions from a JSON file.\n",
    "\n",
    "        Args:\n",
    "            path (str): the file path to the JSON file containing the questions.\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.questions = {}\n",
    "        self.lut = []\n",
    "\n",
    "        for question in data:\n",
    "            self.lut.append(question['_id'])\n",
    "            self.questions[question['_id']] = self.Question(\n",
    "                question['_id'],\n",
    "                question['question'],\n",
    "                question['answer'])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        return the number of questions.\n",
    "\n",
    "        Returns:\n",
    "            int: the number of questions.\n",
    "        \"\"\"\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, key: int|str):\n",
    "        \"\"\"\n",
    "        retrieve a question by index or id.\n",
    "\n",
    "        Args:\n",
    "            key (int | str): the idx or id of the question.\n",
    "\n",
    "        Returns:\n",
    "            DevQuestions.Question: the question corresponding to the given index or id.\n",
    "        \"\"\"\n",
    "        if isinstance(key, int):    # if key is an index\n",
    "            return self.questions.get(self.lut[key])\n",
    "        elif isinstance(key, str):  # if key is an id (str)\n",
    "            return self.questions[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA:\n",
    "    def __init__(self, path_to_dev: str, model: str='gpt-3.5-turbo', system_prompt: str=''):\n",
    "        \"\"\"\n",
    "        initialize a QA instance.\n",
    "\n",
    "        Args:\n",
    "            path_to_dev (str): The file path to the development question.\n",
    "            model (str, optional): The model to use. Defaults to 'gpt-3.5-turbo'.\n",
    "            system_prompt (str, optional): The system prompt to use. Defaults to ''.\n",
    "        \"\"\"\n",
    "        self.client = openai.Client()\n",
    "        self.system_prompt = system_prompt\n",
    "        self.dev = DevQuestions(path_to_dev)\n",
    "        self.contexts = {}\n",
    "        self.model = model\n",
    "\n",
    "    def batch_by_context_no_context(self,\n",
    "                                context_name: str,\n",
    "                                path: str,\n",
    "                                num_questions: int=0,\n",
    "                                write_debug_file: bool=True) -> str:\n",
    "        \"\"\"\n",
    "        constructs a batch of requests for the openai API and saves it to a .jsonl file.\n",
    "        identifies the questions by id given in the context, this version does NOT add any context to the prompt, \n",
    "        only asks the question.\n",
    "\n",
    "        Args:\n",
    "            context_name (str): the name of the context to use.\n",
    "            path (str): the directory path to save the .jsonl file.\n",
    "            num_questions (int, optional): the number of questions to ask - defaults to 0 (all questions).\n",
    "            write_debug_file (bool, optional): whether to write the prompts to a debug file - defaults to true.\n",
    "\n",
    "        Returns:\n",
    "            str: the path to the jsonl file containing the batched requests.\n",
    "        \"\"\"\n",
    "        if context_name not in self.contexts:\n",
    "            raise ValueError('Context not found!')\n",
    "\n",
    "        num_questions = self._validate_num_questions(num_questions, context_name)\n",
    "\n",
    "        print(f\"Creating batch for {num_questions} questions from '{context_name}' context...\")\n",
    "\n",
    "        location = f'{path}/{context_name}-no_context/{self._generate_date_string()}/'\n",
    "\n",
    "        try:\n",
    "            os.makedirs(location, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"error creating directory '{location}': {e}\")\n",
    "\n",
    "        batch = {}\n",
    "        prompts = []\n",
    "        q_ids = []\n",
    "\n",
    "        for c, (q_id, _) in enumerate(self.contexts[context_name].items()):\n",
    "            if c == num_questions:\n",
    "                break\n",
    "\n",
    "            question = self.dev[q_id]\n",
    "            prompt = self._create_prompt([], question.question)  # No context added\n",
    "            prompts.append(prompt)\n",
    "            q_ids.append(q_id)\n",
    "            batch[q_id] = self._build_request_dict(q_id, prompt)\n",
    "\n",
    "            self._save_to_jsonl(batch, location, 'submitted_batch')\n",
    "\n",
    "        if write_debug_file:\n",
    "            self._write_prompts_to_file(q_ids, prompts, location, 'debug')\n",
    "\n",
    "        return f'{location}submitted_batch.jsonl'\n",
    "\n",
    "    def batch_by_context(self,\n",
    "                         context_name: str,\n",
    "                         path: str,\n",
    "                         num_questions: int=0,\n",
    "                         k: int=0,\n",
    "                         write_debug_file: bool=True) -> str:\n",
    "        \"\"\"\n",
    "        constructs a batch of requests for the OpenAI API and saves it to a .jsonl file.\n",
    "        identifies the questions by id given in the context.\n",
    "\n",
    "        Args:\n",
    "            context_name (str): the name of the context.\n",
    "            location (str): the directory path to save the .jsonl file.\n",
    "            file_name (str): the name of the jsonl file.\n",
    "            num_questions (int, optional): the number of questions to ask -defaults to 0 (all questions).\n",
    "            k (int, optional): the number of contexts to include from the context dictionary - defaults to 0 (all available contexts).\n",
    "            write_debug_file (bool, optional): whether to write the prompts to a debug file. defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            str: the path to the jsonl file containing the batched requests.\n",
    "        \"\"\"\n",
    "        if context_name not in self.contexts:\n",
    "            raise ValueError('Context not found!')\n",
    "        \n",
    "        num_questions = self._validate_num_questions(num_questions, context_name)\n",
    "\n",
    "        print(f\"Creating batch for {num_questions} questions from '{context_name}'.\")\n",
    "\n",
    "        location = f'{path}/{context_name}-k_{k}/{self._generate_date_string()}/'\n",
    "\n",
    "        try:\n",
    "            os.makedirs(location, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory '{location}': {e}\")\n",
    "\n",
    "        batch = {}\n",
    "        k_adjustments = 0\n",
    "        prompts = []\n",
    "        q_ids = []\n",
    "\n",
    "        for c, (q_id, contexts) in enumerate(self.contexts[context_name].items()):\n",
    "            if c == num_questions:\n",
    "                break\n",
    "\n",
    "            k_, k_adjustments = self._adjust_k(k, contexts, k_adjustments)\n",
    "            contexts = contexts[:k_]\n",
    "\n",
    "            prompt = self._create_prompt(contexts, self.dev[q_id].question)\n",
    "            prompts.append(prompt)\n",
    "            q_ids.append(q_id)\n",
    "            batch[q_id] = self._build_request_dict(q_id, prompt)\n",
    "\n",
    "        self._print_k_adjustments_warning(k_adjustments, len(batch))\n",
    "        self._save_to_jsonl(batch, location, 'submitted_batch')\n",
    "        \n",
    "        if write_debug_file: self._write_prompts_to_file(q_ids, prompts, location, 'debug')\n",
    "\n",
    "        return f'{location}submitted_batch.jsonl'\n",
    "\n",
    "    def batch_by_structured_mixed_context(self,\n",
    "                                          context_name1: str,\n",
    "                                          context_name2: str,\n",
    "                                          path: str,\n",
    "                                          num_questions: int=0,\n",
    "                                          k1: int=0,\n",
    "                                          k2: int=0,\n",
    "                                          position: str=\"first\",\n",
    "                                          write_debug_file: bool=True) -> str:\n",
    "        \"\"\"\n",
    "        constructs a batch of requests for the OpenAI API and saves it to a .jsonl file.\n",
    "        identifies the questions by id given in the context.\n",
    "        mixed context from two different contexts given the position and number of contexts to include from each context.\n",
    "\n",
    "        Args:\n",
    "            context_name1 (str): the name of the first context.\n",
    "            context_name2 (str): the name of the second context.\n",
    "            path (str): the directory path to save the .jsonl file.\n",
    "            num_questions (int, optional): the number of questions to ask -defaults to 0 (all questions).\n",
    "            k1 (int, optional): the number of contexts to include from the first context dictionary - defaults to 0 (all available contexts).\n",
    "            k2 (int, optional): the number of contexts to include from the second context dictionary - defaults to 0 (all available contexts).\n",
    "            position (str, optional): the position to place contexts from the first context in relation to the second context - defaults to \"first\".\n",
    "            write_debug_file (bool, optional): whether to write the prompts to a debug file. defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            str: the path to the jsonl file containing the batched requests.\n",
    "        \"\"\"\n",
    "        if set(self.contexts[context_name1].keys()) != set(self.contexts[context_name2].keys()):\n",
    "            raise ValueError('Context keys do not match!')\n",
    "\n",
    "        if context_name1 not in self.contexts or context_name2 not in self.contexts:\n",
    "            raise ValueError('Context not found!')\n",
    "\n",
    "        num_questions = self._validate_num_questions(num_questions, context_name1)\n",
    "\n",
    "        print(f\"Creating batch for {num_questions} questions from '{context_name1}' and '{context_name2}' contexts...\")\n",
    "\n",
    "        location = f'{path}/{context_name1}-{context_name2}-k1_{k1}-k2_{k2}_{position}/{self._generate_date_string()}/'\n",
    "\n",
    "        try:\n",
    "            os.makedirs(location, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory '{location}': {e}\")\n",
    "\n",
    "        batch = {}\n",
    "        k1_adjustments = 0\n",
    "        k2_adjustments = 0\n",
    "        prompts = []\n",
    "        q_ids = []\n",
    "\n",
    "        for c, (q_id, contexts1) in enumerate(self.contexts[context_name1].items()):\n",
    "            if c == num_questions:\n",
    "                break\n",
    "\n",
    "            question = self.dev[q_id]\n",
    "            contexts2 = self.contexts[context_name2].get(q_id, [])\n",
    "\n",
    "            k1_, k1_adjustments = self._adjust_k(k1, contexts1, k1_adjustments)\n",
    "            k2_, k2_adjustments = self._adjust_k(k2, contexts2, k2_adjustments)\n",
    "\n",
    "            combined_contexts = self._combine_contexts(contexts1[:k1_], contexts2[:k2_*2], position)\n",
    "            prompt = self._create_prompt(combined_contexts, question.question)\n",
    "            prompts.append(prompt)\n",
    "            q_ids.append(q_id)\n",
    "            batch[q_id] = self._build_request_dict(q_id, prompt)\n",
    "\n",
    "            self._print_k_adjustments_warning(k1_adjustments, len(batch), 'k1')\n",
    "            self._print_k_adjustments_warning(k2_adjustments, len(batch), 'k2')\n",
    "            self._save_to_jsonl(batch, location, 'submitted_batch')\n",
    "\n",
    "        if write_debug_file: self._write_prompts_to_file(q_ids, prompts, location, 'debug')\n",
    "\n",
    "        return f'{location}submitted_batch.jsonl'\n",
    "        \n",
    "    def submit_batch(self, batch_path: str, description: str) -> str:\n",
    "        \"\"\"\n",
    "        submit a batch job to the openai api. saves the batch job ID and description to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            batch_path (str): the path to the batch jsonl file to submit\n",
    "            description (str): description of the batch job, saved to the csv file.\n",
    "        \"\"\"\n",
    "        batch_input_file = self.client.files.create(\n",
    "            file=open(batch_path, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        batch_input_file_id = batch_input_file.id\n",
    "\n",
    "        batch_job = self.client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"file_name\": batch_path}\n",
    "        )\n",
    "        batch_job_id = batch_job.id\n",
    "\n",
    "        print(f\"Batch job '{batch_path}' submitted with ID: {batch_job_id}\")\n",
    "        \n",
    "        with open('batches.csv', 'a', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow([batch_job_id, description, batch_path])\n",
    "        \n",
    "        return batch_job_id\n",
    "    \n",
    "    def submit_split_batch(self, batch_path: str, description: str, num_splits: int) -> list:\n",
    "        \"\"\"\n",
    "        split a .jsonl file into smaller parts and submit them as separate batch jobs.\n",
    "\n",
    "        Args:\n",
    "            batch_path (str): the path to the .jsonl file to split and submit.\n",
    "            description (str): the description to use for the batch jobs, with the part number appended.\n",
    "            num_splits (int): the number of splits to create\n",
    "        \"\"\"\n",
    "        split_file_paths = self.split_jsonl_file(batch_path, num_splits)\n",
    "        batch_job_ids = []\n",
    "\n",
    "        for index, split_file_path in enumerate(split_file_paths):\n",
    "            split_description = f\"{description} -part {index + 1}\"\n",
    "            batch_job_id = self.submit_batch(split_file_path, split_description)\n",
    "            batch_job_ids.append(batch_job_id)\n",
    "            while not self.is_batch_done(batch_job_id) and not self.is_batch_failed(batch_job_id):\n",
    "                time.sleep(30)\n",
    "\n",
    "            with open(f'{os.path.dirname(batch_path)}/ids.txt', 'a') as f:\n",
    "                f.write(f'{batch_job_id}\\n')\n",
    "\n",
    "        return batch_job_ids\n",
    "    \n",
    "    def split_jsonl_file(self, jsonl_file_path: str, num_splits: int) -> list:\n",
    "        \"\"\"\n",
    "        split a .jsonl file into smaller parts. each part is saved as a separate file.\n",
    "        a bit of a hacky solution, but it works.\n",
    "\n",
    "        Args:\n",
    "            jsonl_file_path (str): the path to the .jsonl file to split\n",
    "            num_splits (int): the number of splits to create.\n",
    "        \"\"\"\n",
    "        base_dir = os.path.dirname(jsonl_file_path)\n",
    "        base_name = os.path.basename(jsonl_file_path).split('.')[0]\n",
    "        with open(jsonl_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        total_lines = len(lines)\n",
    "        lines_per_split = total_lines // num_splits\n",
    "        split_file_paths = []\n",
    "\n",
    "        for i in range(num_splits):\n",
    "            split_file_name = f\"{base_name}_part_{i + 1}.jsonl\"\n",
    "            split_file_path = os.path.join(base_dir, split_file_name)\n",
    "            split_lines = lines[i*lines_per_split:(i+1)* lines_per_split] if i < num_splits-1 else lines[i*lines_per_split:]\n",
    "            with open(split_file_path, 'w') as split_file:\n",
    "                split_file.writelines(split_lines)\n",
    "            split_file_paths.append(split_file_path)\n",
    "\n",
    "        return split_file_paths\n",
    "    \n",
    "    def is_batch_done(self, batch_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        check if a batch job is completed\n",
    "\n",
    "        args:\n",
    "            batch_id (str): the ID of the batch job to check.\n",
    "\n",
    "        returns:\n",
    "            bool: true if the batch job is completed, otherwise False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_status = self.client.batches.retrieve(batch_id)\n",
    "            status = batch_status.status\n",
    "            return status == \"completed\"\n",
    "        except Exception as e:\n",
    "            print(f\"error checking batch job {batch_id}: {e}\")\n",
    "            return False\n",
    "        \n",
    "    def is_batch_failed(self, batch_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        check if a batch job has failed\n",
    "\n",
    "        args:\n",
    "            batch_id (str): The ID of the batch job to check.\n",
    "\n",
    "        returns:\n",
    "            bool: True if the batch job has failed, otherwise False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_status = self.client.batches.retrieve(batch_id)\n",
    "            status = batch_status.status\n",
    "            return status in [\"failed\", \"expired\", \"canceled\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking batch job {batch_id}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_batch_job(self, batch_job_id: str, csv_file: str='batches.csv') -> dict:\n",
    "        \"\"\"\n",
    "        load batch job details from a csv file using the batch job ID.\n",
    "        this is for retrieving \n",
    "\n",
    "        args:\n",
    "            batch_job_id (str): the ID of the batch job to look up\n",
    "            csv_file (str, optional): the path to the CSV file containing batch job details - dfaults to 'batches.csv'\n",
    "\n",
    "        returns:\n",
    "            dict: the dictionary containing the batch job details.\n",
    "        \"\"\"\n",
    "        with open(csv_file, 'r', newline='') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            for row in csvreader:\n",
    "                if row[0] == batch_job_id:\n",
    "                    return {\n",
    "                        \"batch_job_id\": row[0],\n",
    "                        \"description\": row[1],\n",
    "                        \"jsonl_file_path\": row[2]\n",
    "                    }\n",
    "        raise ValueError(f\"batch job id '{batch_job_id}' not found in csvv.\")\n",
    "\n",
    "    def check_batches(self, batch_ids: list, csv_file: str='batches.csv') -> dict:\n",
    "        \"\"\"\n",
    "        check the status of multiple batch jobs and save the results to a CSV file.\n",
    "\n",
    "        args:\n",
    "            batch_ids (list): a list of batch job ids to check.\n",
    "            csv_file (str, optional): the path to the CSV file containing batch job details - defaults to 'batches.csv'\n",
    "\n",
    "        returns:\n",
    "            dict: a dictionary containing the results of the batch jobs\n",
    "        \"\"\"\n",
    "        all_results = {}\n",
    "\n",
    "        for batch_id in batch_ids:\n",
    "            batch_details = self.load_batch_job(batch_id, csv_file)\n",
    "            jsonl_file_path = batch_details[\"jsonl_file_path\"]\n",
    "            save_results_to = os.path.dirname(jsonl_file_path)\n",
    "            batch_name = os.path.basename(jsonl_file_path).split('.')[0]\n",
    "\n",
    "            try:\n",
    "                batch_status = self.client.batches.retrieve(batch_id)\n",
    "                status = batch_status.status\n",
    "                \n",
    "                if status == \"completed\":\n",
    "                    results_file_id = batch_status.output_file_id\n",
    "                    results_file = self.client.files.content(results_file_id)\n",
    "\n",
    "                    results_file_path = os.path.join(save_results_to, f\"{batch_name}_rslt.jsonl\")\n",
    "                    results = []\n",
    "                    with open(results_file_path, 'wb') as output_file:\n",
    "                        for line in results_file.iter_lines():\n",
    "                            if line.strip():\n",
    "                                json_line = json.loads(line)\n",
    "                                results.append(json_line)\n",
    "                                output_file.write((json.dumps(json_line) + '\\n').encode('utf-8'))\n",
    "\n",
    "                    print(f\"batch job results for {batch_id} - {jsonl_file_path} saved to {results_file_path}\")\n",
    "\n",
    "                    try:\n",
    "                        os.makedirs(os.path.join(save_results_to, 'csv'), exist_ok=True)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error creating directory '{save_results_to}/csv': {e}\")\n",
    "\n",
    "                    results_file_path = os.path.join(save_results_to, f\"csv/{batch_name}_rslt.csv\")\n",
    "                    self.write_csv(jsonl_file_path, results_file_path, results)\n",
    "                    all_results[batch_id] = results\n",
    "                elif status in [\"failed\", \"expired\", \"canceled\"]:\n",
    "                    print(f\"batch job {batch_id} / {jsonl_file_path} failed with status: {status}\")\n",
    "                else:\n",
    "                    print(f\"batch job {batch_id} / {jsonl_file_path} is not yet completed. Current status: {status}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"errror checking batch job {batch_id} / {jsonl_file_path}: {e}\")\n",
    "        \n",
    "        return all_results \n",
    "\n",
    "    def set_system_prompt(self, prompt: str):\n",
    "        \"\"\"\n",
    "        sets the system prompt for gpt to use\n",
    "\n",
    "        args:\n",
    "            prompt (str): the prompt to set.\n",
    "        \"\"\"\n",
    "        if not isinstance(prompt, str):\n",
    "            raise TypeError('prompt must be a string!')\n",
    "        self.system_prompt = prompt\n",
    "\n",
    "    def get_context(self, name: str) -> dict:\n",
    "        \"\"\"\n",
    "        get a context by name\n",
    "\n",
    "        args:\n",
    "            name (str): the name of the context.\n",
    "\n",
    "        returns:\n",
    "            dict: the context dictionary.\n",
    "        \"\"\"\n",
    "        if not isinstance(name, str) or name not in self.contexts:\n",
    "            raise ValueError('Context not found!')\n",
    "        return self.contexts.get(name)\n",
    "\n",
    "    def print_context_names(self):\n",
    "        \"\"\"\n",
    "        print the names of all contexts available\n",
    "        \"\"\"\n",
    "        for name in self.contexts.keys():\n",
    "            print(name)\n",
    "\n",
    "    def add_context(self, context: dict, name: str):\n",
    "        \"\"\"\n",
    "        add a context. it should be a dictionary with the following structure:\n",
    "        {\n",
    "            'question_idA': ['contextA1', 'contextA2', ...],\n",
    "            'question_idB': ['contextB1', 'contextB2', ...],\n",
    "            ...\n",
    "        }\n",
    "\n",
    "        args:\n",
    "            context (dict): the context to add.\n",
    "            name (str): the name of the context.\n",
    "        \"\"\"\n",
    "        if not isinstance(context, dict):\n",
    "            raise TypeError('context must be a dictionary!')\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError('name must be a string!')\n",
    "        if name in self.contexts:\n",
    "            raise ValueError('context already exists!')\n",
    "        if not all(isinstance(k, str) and isinstance(v, list) for k, v in context.items()):\n",
    "            raise TypeError('invalid format!')\n",
    "\n",
    "        self.contexts[name] = context\n",
    "\n",
    "    def write_csv(self, batch_file: str, target: str, responses: list):\n",
    "        \"\"\"\n",
    "        Write responses to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            path_to_batch (str): The file path to the batch JSONL file.\n",
    "            responses (list): List of response dictionaries from the batch request.\n",
    "            csv_path (str): The file path to save the CSV file.\n",
    "        \"\"\"\n",
    "        with open(batch_file, 'r', encoding='utf-8') as batch:\n",
    "            batch_prompts = {}\n",
    "            for line in batch:\n",
    "                request = json.loads(line)\n",
    "                q_id = request['custom_id']\n",
    "                prompt = request['body']['messages'][-1]['content']\n",
    "                batch_prompts[q_id] = prompt\n",
    "\n",
    "        with open(target, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"Question ID\", \"GPT Response\", \"Ground Truth Answer\", \"Prompt\"])\n",
    "\n",
    "            for response in responses:\n",
    "                q_id = response['custom_id']\n",
    "                gpt_response = response['response']['body']['choices'][0]['message']['content']\n",
    "                question = self.dev[q_id]\n",
    "                ground_truth_answer = question.answer\n",
    "                prompt = batch_prompts[q_id]\n",
    "\n",
    "                writer.writerow([q_id, gpt_response, ground_truth_answer, prompt])\n",
    "\n",
    "    def _generate_date_string(self):\n",
    "        \"\"\"\n",
    "        generatw a date string in the format YYYYMMDD_HHMMSS.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated date string.\n",
    "        \"\"\"\n",
    "        date_string = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return date_string\n",
    "    \n",
    "    def _validate_num_questions(self, num_questions: int, context_name: str) -> int:\n",
    "        \"\"\"\n",
    "        Validate the number of questions to be processed.\n",
    "\n",
    "        Args:\n",
    "            num_questions (int): The number of questions to ask. If 0, all questions are processed.\n",
    "            context_name (str): The name of the context.\n",
    "\n",
    "        Returns:\n",
    "            int: The validated number of questions to process.\n",
    "        \"\"\"\n",
    "        total_questions = len(self.contexts[context_name])\n",
    "        if num_questions <= 0 or num_questions > total_questions:\n",
    "            num_questions = total_questions\n",
    "            print('Using all available questions.')\n",
    "        return num_questions\n",
    "\n",
    "    def _combine_contexts(self, contexts1: list[str], contexts2: list[str], position: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Combine two lists of contexts based on the specified position.\n",
    "\n",
    "        Args:\n",
    "            contexts1 (list[str]): The first list of contexts.\n",
    "            contexts2 (list[str]): The second list of contexts.\n",
    "            position (str): The position to place contexts from contexts1 in relation to contexts2.\n",
    "                            Can be \"first\", \"middle\", or \"last\".\n",
    "\n",
    "        Returns:\n",
    "            list[str]: The combined list of contexts.\n",
    "        \"\"\"\n",
    "        if position == \"first\":\n",
    "            return contexts1 + contexts2\n",
    "        elif position == \"middle\":\n",
    "            half = len(contexts2) // 2\n",
    "            return contexts2[:half] + contexts1 + contexts2[half:]\n",
    "        elif position == \"last\":\n",
    "            return contexts2 + contexts1\n",
    "        else:\n",
    "            raise ValueError('Invalid position value. It should be \"first\", \"middle\", or \"last\".')\n",
    "\n",
    "    def _build_request_dict(self, q_id: str, prompt: str) -> dict:\n",
    "        \"\"\"\n",
    "        Build a request dictionary for the OpenAI API.\n",
    "\n",
    "        Args:\n",
    "            q_id (str): The question ID.\n",
    "            prompt (str): The prompt to be used in the API request.\n",
    "\n",
    "        Returns:\n",
    "            dict: The request dictionary.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"custom_id\": q_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": self.model, \n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": 60\n",
    "                }\n",
    "            }   \n",
    "\n",
    "    def _create_prompt(self, contexts:list[str], question:str) -> str:\n",
    "        \"\"\"\n",
    "        create a prompt from the contexts and question.\n",
    "\n",
    "        args:\n",
    "            contexts (list[str]): the contexts\n",
    "            question (str): the questionn\n",
    "        \"\"\"\n",
    "        prompt = ''\n",
    "        for context in contexts:\n",
    "            prompt += context + '\\n\\n'\n",
    "        prompt += question\n",
    "        return prompt\n",
    "\n",
    "    def _adjust_k(self, k: int, contexts: list[str], k_adjustments: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Adjust the value of k based on the length of the contexts list.\n",
    "\n",
    "        Args:\n",
    "            k (int): The desired number of contexts to include.\n",
    "            contexts (list[str]): The list of contexts available.\n",
    "            k_adjustments (int): The current count of k adjustments.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The adjusted value of k and the updated count of k adjustments.\n",
    "        \"\"\"\n",
    "        if k == 0 or k > len(contexts):\n",
    "            k_adjustments += 1\n",
    "            k = len(contexts)\n",
    "        return k, k_adjustments\n",
    "    \n",
    "    def _print_k_adjustments_warning(self, k_adjustments: int, total: int, k_label: str = 'k') -> None:\n",
    "        \"\"\"\n",
    "        Print a warning if there were adjustments made to the value of k.\n",
    "\n",
    "        Args:\n",
    "            k_adjustments (int): The number of adjustments made to k.\n",
    "            num_batches (int): The number of batches processed.\n",
    "            k_label (str, optional): The label for k. Defaults to 'k'.\n",
    "        \"\"\"\n",
    "        if k_adjustments:\n",
    "            print(f\"WARNING: total adjustments of '{k_label}': {k_adjustments} of {total}\")\n",
    "            print(f\"This warning is because the specified {k_label} is outside the valid range or exceeds available contexts.\")\n",
    "\n",
    "    def _write_prompts_to_file(self, ids: list[str], prompts: list[str], path: str, file_name: str) -> None:\n",
    "        \"\"\"\n",
    "        write prompts to a text file.\n",
    "\n",
    "        Args:\n",
    "            prompts (list[str]): the prompts to write.\n",
    "            file_path (str): the file path to write to.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(f'{path}/{file_name}.txt', 'w') as file:\n",
    "            for i, prompt in enumerate(prompts):\n",
    "                file.write(f\"Question ID: {ids[i]}\\n\")\n",
    "                file.write(f\"{prompt}\\n\\n\")\n",
    "                file.write('--------------------------------------\\n\\n')\n",
    "\n",
    "    def _save_to_jsonl(self, batch: dict, path: str, file_name: str) -> None:\n",
    "        \"\"\"\n",
    "        save the batch requests to a JSONL file.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): The batch of requests to save.\n",
    "            path (str): The file path to save the .jsonl file.\n",
    "        \"\"\"\n",
    "        with open(f'{path}/{file_name}.jsonl', 'w', encoding='utf-8') as f:\n",
    "            for request in batch.values():\n",
    "                f.write(json.dumps(request) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = load_json('contexts/results_10.json')\n",
    "\n",
    "def remove_duplicates(data):\n",
    "    for key, value in data.items():\n",
    "        unique_entries = []\n",
    "        seen_entries = set()\n",
    "        for entry in value:\n",
    "            cleaned_entry = ''.join(entry.split()).lower()\n",
    "            if cleaned_entry not in seen_entries:\n",
    "                unique_entries.append(entry)\n",
    "                seen_entries.add(cleaned_entry)\n",
    "        data[key] = unique_entries\n",
    "    return data\n",
    "\n",
    "keys = list(top10.keys())\n",
    "\n",
    "top10 = remove_duplicates(top10)\n",
    "\n",
    "for key, value in top10.items():\n",
    "    for i, _ in enumerate(value):\n",
    "        top10[key][i] = value[i].replace('\\n\\n', '\\n')\n",
    "\n",
    "save_json('contexts/top10_no_duplicates.json', top10)    \n",
    "\n",
    "_dev = load_json('dataset/_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = {}\n",
    "\n",
    "for question in _dev:  # iterate over each question in the full dataset\n",
    "    question_id = question['_id'] # get the id of the question\n",
    "    if question_id not in keys: continue  # skip if the question is not part of the ones from top10.json\n",
    "\n",
    "    full_context = question['context']  # get all contexts of the question\n",
    "    supporting_facts = question['supporting_facts']  # get the supporting facts of the question\n",
    "\n",
    "    context_list = []  # initialize an empty list to store context strings\n",
    "    oracle[question_id] = []  # initialize an empty list for the current question id in gold_context dict\n",
    "\n",
    "    for idx_context, context in enumerate(full_context):  # iterate over each given context in the dev dataset\n",
    "        title = context[0]  # get the title of the context\n",
    "        if title not in [sf[0] for sf in supporting_facts]: continue  # skip if the title is not in the supporting facts\n",
    "        # because we only want to include the context if it is a supporting fact\n",
    "\n",
    "        string = title + '\\n' # start the context string with the title and a newline\n",
    "        for i, c in enumerate(context[1]): # iterate over each sentence in the context\n",
    "            string += c  # append the sentence to the context string\n",
    "\n",
    "        oracle[question_id].append(string) # add the context string to the list for the currennt question id\n",
    "\n",
    "save_json('contexts/oracle.json', oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negatives = {}\n",
    "\n",
    "for question in _dev: # iterate over each question in the _dev dataset\n",
    "    question_id = question['_id'] # get the id of the question\n",
    "    if question_id not in keys: continue # skip if the question is not part of the ones from top10.json\n",
    "    \n",
    "    hard_negatives[question_id] = []  # initialize an empty list for the current question id in hard_negatives\n",
    "    supporting_facts = question['supporting_facts'] # get the supporting facts of the question\n",
    "\n",
    "    titles_top10 = [t.split('\\n')[0] for t in top10[question_id]] # get the titles from the top10 for the current question id\n",
    "\n",
    "    for idx_title, title in enumerate(titles_top10): # iterate over each title in the top10 titles\n",
    "        if title not in [sf[0] for sf in supporting_facts]: # if the title is not in the supporting facts\n",
    "            hard_negatives[question_id].append(top10[question_id][idx_title]) # we only want to include the context if it is not a\n",
    "            # supporting fact but seems useful according to the retriever\n",
    "\n",
    "save_json('contexts/hard_negatives.json', hard_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomly_drawn = {}\n",
    "complete_corpus = list(load_json('dataset/wiki_musique_corpus.json').values())\n",
    "\n",
    "print(len(complete_corpus))\n",
    "\n",
    "num_contexts = 10\n",
    "\n",
    "for key in keys:\n",
    "    randomly_drawn[key] = []\n",
    "    for _ in range(10):\n",
    "        choice = random.choice(complete_corpus)\n",
    "        title = choice['title']\n",
    "        text = choice['text']\n",
    "        randomly_drawn[key].append(title + '\\n' + text)\n",
    "\n",
    "save_json('contexts/random.json', randomly_drawn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negatives = load_json('contexts/hard_negatives.json')\n",
    "oracle = load_json('contexts/oracle.json')\n",
    "randomly_drawn = load_json('contexts/random.json')\n",
    "top10 = load_json('contexts/top10_no_duplicates.json')\n",
    "gibberish = load_json('contexts/gibberish.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT:\n",
      "Answer in a concise way. NO full sentences! As few words as possible! For example just a date, the name of a place, the name of a person, a number, a yes or no, etc.\n",
      "\n",
      "CONTEXTS:\n",
      "hard_negatives\n",
      "oracle\n",
      "random\n",
      "top10\n",
      "gibberish\n"
     ]
    }
   ],
   "source": [
    "system_prompt = 'Answer in a concise way. NO full sentences! As few words as possible! For example just a date, the name of a place, the name of a person, a number, a yes or no, etc.'\n",
    "\n",
    "qa = QA('dataset/_dev.json', model='gpt-3.5-turbo', system_prompt=system_prompt)\n",
    "\n",
    "qa.add_context(hard_negatives, 'hard_negatives')\n",
    "qa.add_context(oracle, 'oracle')\n",
    "qa.add_context(randomly_drawn, 'random')\n",
    "qa.add_context(top10, 'top10')\n",
    "qa.add_context(gibberish, 'gibberish')\n",
    "\n",
    "# print system prompt\n",
    "print('SYSTEM PROMPT:')\n",
    "print(qa.system_prompt)\n",
    "\n",
    "# print context names\n",
    "print('\\nCONTEXTS:')\n",
    "qa.print_context_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actually_prompt = True\n",
    "num_questions = 0\n",
    "description = 'random injected noise, serious run 1'\n",
    "\n",
    "K_top10 = [1, 3, 5]\n",
    "positions = ['first', 'middle', 'last']\n",
    "\n",
    "current_ids = []\n",
    "\n",
    "for k in K_top10:\n",
    "    for position in positions:\n",
    "        path = f'results/injected_noise/{k}_{position}'\n",
    "        batch_path = qa.batch_by_structured_mixed_context('top10', 'random', 'results', num_questions=num_questions, k1=k, k2=2, position=position, write_debug_file=True)\n",
    "        if actually_prompt:\n",
    "            batch_job_ids = qa.submit_split_batch(batch_path, f'{description}, k={k}, pos={position}', 10)\n",
    "            current_ids.extend(batch_job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa.check_batches(current_ids, 'batches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.concatenate_csv_files('./results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HARD NEGATIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.actually_prompt = True\n",
    "actually_prompt = True\n",
    "\n",
    "path = 'results/hard_negatives'\n",
    "\n",
    "K = [1, 3, 5]\n",
    "\n",
    "for k in K:\n",
    "    if k > 1: break\n",
    "    file_name = f'e4_qa_hard_negatives_k{k}'\n",
    "    batch_loc_name = qa.batch_by_context('hard_negatives', path, file_name, num_questions=1, k=k, write_debug_file=True)\n",
    "    if actually_prompt:\n",
    "        responses = qa.ask_batch(batch_loc_name[0], batch_loc_name[1], f'QA on hard negatives with k={k}')\n",
    "        qa.write_csv(batch_loc_name[0], batch_loc_name[1], responses)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batch for 5 questions from 'random'.\n",
      "results/submitted_batch.jsonl\n",
      "Creating batch for 5 questions from 'random'.\n",
      "results/submitted_batch.jsonl\n",
      "Creating batch for 5 questions from 'random'.\n",
      "results/submitted_batch.jsonl\n",
      "Creating batch for 5 questions from 'random'.\n",
      "results/submitted_batch.jsonl\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "actually_prompt = True\n",
    "K = [1, 3, 5, 10]\n",
    "\n",
    "current_ids = []\n",
    "\n",
    "description = 'just random noise'\n",
    "\n",
    "for k in K:\n",
    "    path = f'results/'\n",
    "    batch_loc_name = qa.batch_by_context('random', path, num_questions=5, k=k, write_debug_file=True)\n",
    "    print(batch_loc_name)\n",
    "    # if actually_prompt:\n",
    "    #         batch_job_ids = qa.submit_split_batch(batch_loc_name[0], description, 2)\n",
    "    #         current_ids.extend(batch_job_ids)\n",
    "\n",
    "print(current_ids)\n",
    "\n",
    "result = qa.check_batches(current_ids, 'batches.csv')\n",
    "#concatenate_csv_files('./results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
