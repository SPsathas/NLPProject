{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:03:39.179116Z",
     "start_time": "2024-06-11T07:03:37.634119Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import random\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_json(path: str) -> Any:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def save_json(path: str, data: Any) -> None:\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `DevQuestions` class loads and manages questions from a json file. it contains a nested `Question` class for individual questions with an id, question text, and answer. methods are provided to get the number of questions and access questions by index or id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:03:40.245118Z",
     "start_time": "2024-06-11T07:03:40.215118Z"
    }
   },
   "outputs": [],
   "source": [
    "class DevQuestions:\n",
    "    class Question:\n",
    "        def __init__(self, identifier: str, question: str, answer: str):\n",
    "            \"\"\"\n",
    "            initialize a Question instance.\n",
    "\n",
    "            Args:\n",
    "                identifier (str): the identifier for the question.\n",
    "                question (str): the question text.\n",
    "                answer (str): the answer text.\n",
    "            \"\"\"\n",
    "            self.id = identifier\n",
    "            self.question = question\n",
    "            self.answer = answer\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"\n",
    "        initialize the DevQuestions instance by loading questions from a JSON file.\n",
    "\n",
    "        Args:\n",
    "            path (str): the file path to the JSON file containing the questions.\n",
    "        \"\"\"\n",
    "        # load json file\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.questions = {}\n",
    "        self.lut = []\n",
    "\n",
    "        for question in data:\n",
    "            self.lut.append(question['_id'])\n",
    "            self.questions[question['_id']] = self.Question(question['_id'], question['question'], question['answer'])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        return the number of questions.\n",
    "\n",
    "        Returns:\n",
    "            int: the number of questions.\n",
    "        \"\"\"\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, key: int | str):\n",
    "        \"\"\"\n",
    "        retrieve a question by index or id.\n",
    "\n",
    "        Args:\n",
    "            key (int | str): the idx or id of the question.\n",
    "\n",
    "        Returns:\n",
    "            DevQuestions.Question: the question corresponding to the given index or id.\n",
    "        \"\"\"\n",
    "        if isinstance(key, int):\n",
    "            return self.questions.get(self.lut[key])\n",
    "        elif isinstance(key, str):\n",
    "            return self.questions[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `QA` class manages and prompts questions based on given contexts. it loads questions from a specified path and allows setting a system prompt. methods include asking questions by context, prompting the model, setting the system prompt, getting a context by name, and adding new contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:03:43.691166Z",
     "start_time": "2024-06-11T07:03:43.617118Z"
    }
   },
   "outputs": [],
   "source": [
    "class QA:\n",
    "    def __init__(self, path_to_dev: str, model:str='gpt-3.5-turbo', system_prompt:str='', actually_prompt:bool= True):\n",
    "        \"\"\"\n",
    "        initialize a QA instance\n",
    "\n",
    "        args:\n",
    "            path_to_dev (str): the file path to the development question.\n",
    "            system_prompt (str, optional): the system prompt to use. defaults to ''.\n",
    "            actually_prompt (bool, optional): whether to actually prompt the model. defaults to True\n",
    "        \"\"\"\n",
    "        self.client = openai.Client(api_key='sk-proj-OD9UpwZjabyMJ0bPtPnMT3BlbkFJKDkmftX3AjUy6zp6di0P')\n",
    "        self.system_prompt = system_prompt\n",
    "        self.actually_prompt = actually_prompt\n",
    "        self.dev = DevQuestions(path_to_dev)\n",
    "        self.contexts = {}\n",
    "        self.model = model\n",
    "\n",
    "    def ask_by_context(self, context_name:str, num_questions:int=0, k:int=0, write_to_file:tuple[bool, str]=(False, None)) -> dict:\n",
    "        \"\"\"\n",
    "        ask questions by context.\n",
    "\n",
    "        args:\n",
    "            context_name (str): the name of the context\n",
    "            num_questions (int, optional): the number of questions to ask. defaults to 0 (all questions)\n",
    "            k (int, optional): the number of contexts to include in the prompt. defaults to 0, which includes (all available contexts)\n",
    "            write_to_file (tuple[bool, str], optional): write the prompts, responses, and ground truth to a file. defaults to (False, None)\n",
    "\n",
    "        returns:\n",
    "            dict: a dictionary containing prompts, answers, and responses.\n",
    "        \"\"\"\n",
    "        if context_name not in self.contexts:\n",
    "            raise ValueError('context not found!')\n",
    "        if num_questions <= 0 or num_questions > len(self.contexts[context_name]):\n",
    "            num_questions = len(self.contexts[context_name])\n",
    "            print(f\"WARNING: num_questions must be between 1 and {num_questions}! Setting num_questions to {num_questions}.\")\n",
    "\n",
    "        prompts, answers, responses = [], [], []\n",
    "        k_adjustments = 0\n",
    "\n",
    "        for c, (q_id, contexts) in tqdm(enumerate(self.contexts[context_name].items()), total=num_questions, desc=\"Processing Questions\"):\n",
    "            if c == num_questions:\n",
    "                break\n",
    "\n",
    "            k_ = k\n",
    "\n",
    "            question = self.dev[q_id]\n",
    "\n",
    "            if k <= 0 or k > len(contexts):\n",
    "                k_ = len(contexts)\n",
    "                if k_ != 0: k_adjustments += 1\n",
    "\n",
    "            contexts = contexts[:k_]\n",
    "\n",
    "            prompt = self._create_prompt(contexts, question.question)\n",
    "            prompts.append(prompt)\n",
    "            answers.append(question.answer)\n",
    "            responses.append(self.ask(prompt) if self.actually_prompt else 'dummy response! ;)')\n",
    "\n",
    "        if write_to_file[0]: self._write_to_file(write_to_file[1], prompts, responses, answers)\n",
    "\n",
    "        if k_adjustments:\n",
    "            print(f\"WARNING: total adjustments of 'k': {k_adjustments} of {len(prompts)}\")\n",
    "            print(\"this warning is because the specified k is outside the valid range or exceeds available contexts.\")\n",
    "\n",
    "        return {'prompts': prompts, 'answers': answers, 'responses': responses}\n",
    "\n",
    "    def ask_by_mixed_context(self, context_name1:str, context_name2:str, num_questions:int=0, k1:int= 0, k2:int=0, write_to_file:tuple[bool, str]=(False, None)) -> dict:\n",
    "        \"\"\"\n",
    "        ask questions by using the ids from context_name1 and randomly add contexts from context_name2.\n",
    "\n",
    "        args:\n",
    "            context_name1 (str): the name of the first context.\n",
    "            context_name2 (str): the name of the second context.\n",
    "            num_questions (int, optional): the number of questions to ask. defaults to 0 (all questions).\n",
    "            k1 (int, optional): the number of contexts to include from the first context dictionary. defaults to 0 (all available contexts)\n",
    "            k2 (int, optional): the number of contexts to include from the second context dictionary. defaults to 0 (all available contexts)\n",
    "            write_to_file (tuple[bool, str], optional): write the prompts, responses, and ground truth to a file. defaults to (false, none).\n",
    "\n",
    "        returns:\n",
    "            dict: a dictionary containing prompts, answers, and responses.\n",
    "        \"\"\"\n",
    "        if set(list(self.contexts[context_name1].keys())) != set(list(self.contexts[context_name2].keys())):\n",
    "            raise ValueError('context keys do not match!')\n",
    "\n",
    "        if context_name1 not in self.contexts or context_name2 not in self.contexts:\n",
    "            raise ValueError('context not found!')\n",
    "        \n",
    "        if num_questions <= 0 or num_questions > len(self.contexts[context_name1]):\n",
    "            num_questions = len(self.contexts[context_name1])\n",
    "            print(f\"WARNING: num_questions must be between 1 and {num_questions}! Setting num_questions to {num_questions}.\")\n",
    "\n",
    "        prompts, answers, responses = [], [], []\n",
    "        k1_adjustments = 0\n",
    "        k2_adjustments = 0\n",
    "        k1_ = k1\n",
    "        k2_ = k2\n",
    "\n",
    "        for c, (q_id, contexts1) in tqdm(enumerate(self.contexts[context_name1].items()), total=num_questions, desc=\"Processing Questions\"):\n",
    "            if c == num_questions:\n",
    "                break\n",
    "\n",
    "            question = self.dev[q_id]\n",
    "            contexts2 = self.contexts[context_name2].get(q_id, [])\n",
    "\n",
    "            if k1 <= 0 or k1 > len(contexts1):\n",
    "                k1_ = len(contexts1)\n",
    "                if k1 != 0: k1_adjustments += 1\n",
    "            \n",
    "            if k2 <= 0 or k2 > len(contexts2):\n",
    "                k2_ = len(contexts2)\n",
    "                if k2 != 0: k2_adjustments += 1\n",
    "\n",
    "            combined_contexts = contexts1[:k1_] + contexts2[:k2_]\n",
    "            random.shuffle(combined_contexts)\n",
    "\n",
    "            prompt = self._create_prompt(combined_contexts, question.question)\n",
    "            \n",
    "            prompts.append(prompt)\n",
    "            answers.append(question.answer)\n",
    "            responses.append(self.ask(prompt) if self.actually_prompt else 'dummy response! ;)')\n",
    "\n",
    "        if write_to_file[0]: self._write_to_file(write_to_file[1], prompts, responses, answers)\n",
    "\n",
    "        if k1_adjustments:\n",
    "            print(f\"WARNING: total adjustments of 'k1': {k1_adjustments} of {len(prompts)}\")\n",
    "            print(\"this warning is because the specified k1 is outside the valid range or exceeds available contexts.\")\n",
    "        if k2_adjustments:\n",
    "            print(f\"WARNING: total adjustments of 'k2': {k2_adjustments} of {len(prompts)}\")\n",
    "            print(\"this warning is because the specified k2 is outside the valid range or exceeds available contexts.\")\n",
    "\n",
    "        return {'prompts': prompts, 'answers': answers, 'responses': responses}\n",
    "\n",
    "    def ask_by_context_without_context(self, context_name: str, num_questions: int = 0, write_to_file: tuple[bool, str] = (False, None)) -> dict:\n",
    "        \"\"\"\n",
    "        ask questions by context without using the context.\n",
    "\n",
    "        args:\n",
    "            context_name (str): the name of the context.\n",
    "            num_questions (int, optional): the number of questions to ask. defaults to 0 (all questions)\n",
    "            write_to_file (tuple[bool, str], optional): write the prompts, responses, and ground truth to a file. defaults to (False, None)\n",
    "\n",
    "        returns:\n",
    "            dict: a dictionary containing prompts, answers, and responses.\n",
    "        \"\"\"\n",
    "        if context_name not in self.contexts:\n",
    "            raise ValueError('context not found!')\n",
    "        \n",
    "        if num_questions <= 0 or num_questions > len(self.contexts[context_name]):\n",
    "            num_questions = len(self.contexts[context_name])\n",
    "            print(f\"WARNING: num_questions must be between 1 and {num_questions}! Setting num_questions to {num_questions}.\")\n",
    "\n",
    "        prompts, answers, responses = [], [], []\n",
    "\n",
    "        for c, (q_id, _) in enumerate(self.contexts[context_name].items()):\n",
    "            if c == num_questions:\n",
    "                break\n",
    "\n",
    "            question = self.dev[q_id]\n",
    "            prompt = question.question\n",
    "\n",
    "            prompts.append(prompt)\n",
    "            answers.append(question.answer)\n",
    "            responses.append(self.ask(prompt) if self.actually_prompt else 'dummy response! ;)')\n",
    "\n",
    "        if write_to_file[0]:\n",
    "            self._write_to_file(write_to_file[1], prompts, responses, answers)\n",
    "\n",
    "        return {'prompts': prompts, 'answers': answers, 'responses': responses}\n",
    "\n",
    "    def ask(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        ask a question using the given prompt.\n",
    "\n",
    "        args:\n",
    "            prompt (str): the prompt to ask.\n",
    "\n",
    "        returns:\n",
    "            str: the response from the model.\n",
    "        \"\"\"\n",
    "        if not isinstance(prompt, str):\n",
    "            raise TypeError('prompt must be a string!')\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    def set_system_prompt(self, prompt: str):\n",
    "        \"\"\"\n",
    "        sets the system prompt for gpt to use\n",
    "\n",
    "        args:\n",
    "            prompt (str): the prompt to set.\n",
    "        \"\"\"\n",
    "        if not isinstance(prompt, str):\n",
    "            raise TypeError('prompt must be a string!')\n",
    "        self.system_prompt = prompt\n",
    "\n",
    "    def get_context(self, name: str) -> dict:\n",
    "        \"\"\"\n",
    "        get a context by name\n",
    "\n",
    "        args:\n",
    "            name (str): the name of the context.\n",
    "\n",
    "        returns:\n",
    "            dict: the context dictionary.\n",
    "        \"\"\"\n",
    "        if not isinstance(name, str) or name not in self.contexts:\n",
    "            raise TypeError('context not found!')\n",
    "        return self.contexts.get(name)\n",
    "\n",
    "    def print_context_names(self):\n",
    "        \"\"\"\n",
    "        print the names of all contexts available\n",
    "        \"\"\"\n",
    "        for name in self.contexts.keys():\n",
    "            print(name)\n",
    "\n",
    "    def add_context(self, context:dict, name:str):\n",
    "        \"\"\"\n",
    "        add a context. it should be a dictionary with the following structure:\n",
    "        {\n",
    "            'question_idA': ['contextA1', 'contextA2', ...],\n",
    "            'question_idB': ['contextB1', 'contextB2', ...],\n",
    "            ...\n",
    "        }\n",
    "\n",
    "        args:\n",
    "            context (dict): the context to add.\n",
    "            name (str): the name of the context.\n",
    "        \"\"\"\n",
    "        if not isinstance(context, dict):\n",
    "            raise TypeError('context must be a dictionary!')\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError('name must be a string!')\n",
    "        if name in self.contexts:\n",
    "            raise ValueError('context already exists!')\n",
    "        if not all(isinstance(k, str) and isinstance(v, list) for k, v in context.items()):\n",
    "            raise TypeError('invalid format!')\n",
    "\n",
    "        self.contexts[name] = context\n",
    "\n",
    "    def _create_prompt(self, contexts:list[str], question:str) -> str:\n",
    "        \"\"\"\n",
    "        create a prompt from the contexts and question.\n",
    "\n",
    "        args:\n",
    "            contexts (list[str]): the contexts\n",
    "            question (str): the questionn\n",
    "        \"\"\"\n",
    "        prompt = ''\n",
    "        for context in contexts:\n",
    "            prompt += context + '\\n\\n'\n",
    "        prompt += question\n",
    "        return prompt\n",
    "\n",
    "    def _write_to_file(self, file_path:str, prompts:list[str], responses:list[str], answers:list[str]):\n",
    "        \"\"\"\n",
    "        write the prompts, responses, and ground truth to a file for debuging\n",
    "\n",
    "        args:\n",
    "            file_path (str): the file path to write to.\n",
    "            prompts (list[str]): the prompts\n",
    "            responses (list[str]): the responses\n",
    "            answers (list[str]): the groundtruths\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            for i in range(len(prompts)):\n",
    "                file.write(f\"Prompt {i+1}:\\n{prompts[i]}\\n\\n\")\n",
    "                file.write(f\"Response {i+1}:\\n{responses[i]}\\n\\n\")\n",
    "                file.write(f\"Ground Truth {i+1}:\\n{answers[i]}\\n\\n\")\n",
    "                file.write(\"----------------------------------\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top10 contains the top 10 contexts for 1199 questions retrieved by matteo.\n",
    "\n",
    "**in general** contexts should now have the following format:\n",
    "\n",
    "some_context = \n",
    "{\n",
    "  'question_idA': ['contextA1', 'contextA2', ...],\n",
    "  'question_idB': ['contextB1', 'contextB2', ...],\n",
    "  ...\n",
    "}\n",
    "\n",
    "to be compatible with the `QA` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T06:28:40.681363Z",
     "start_time": "2024-06-11T06:28:40.389373Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! cd \n",
    "\n",
    "top10 = load_json('data/results_10.json')\n",
    "keys = list(top10.keys())\n",
    "_dev = load_json('data/_dev.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following two cells builds the gold context and the hard negative contexts by using the supporting facts from the `dev.json` file **for all questions used in top10 by matteo**\n",
    "\n",
    "It looks up the supporting facts / the title of the gold context for each question and adds the sentences with the same title under the context field to the gold context dictionary.\n",
    "\n",
    "The hard negative context is built by iterating over all questions and checking if the title of the contexts retrieved by the contriever show up in the supporting facts of the question. If not, the context is added to the hard negative context dictionary.\n",
    "\n",
    "**NOTE: I DID NOT THOROUGHLY CHECK IF ITS ALWAYS CORRECT, BUT I THINK BOTH CELLS WORK! ONE THING IS THAT THERE ARE OFTEN TWO IDENTICAL CONTEXTS RETRIEVED BY THE CONTRIEVER. THEY HAVE SOME DIFFERENT FORMATTING (SOME HAVE A SPACE AT THE END, SOME NOT ETC.). THIS COULD BE A PROBLEM SINCE THE HARD NEGATIVE CELL COMPARES THE TITLES OF THE CONTEXTS (EG. THE STRING FROM START TO THE FIRST \\n) IN TOP10 WITH THE SUPPORTING FACTS OF THE QUESTIONS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T16:31:18.872825Z",
     "start_time": "2024-06-10T16:31:18.752295Z"
    }
   },
   "outputs": [],
   "source": [
    "gold_context = {}\n",
    "\n",
    "for question in _dev:  # iterate over each question in the full dataset\n",
    "    question_id = question['_id'] # get the id of the question\n",
    "    if question_id not in keys: continue  # skip if the question is not part of the ones from top10.json\n",
    "\n",
    "    full_context = question['context']  # get all contexts of the question\n",
    "    supporting_facts = question['supporting_facts']  # get the supporting facts of the question\n",
    "\n",
    "    context_list = []  # initialize an empty list to store context strings\n",
    "    gold_context[question_id] = []  # initialize an empty list for the current question id in gold_context dict\n",
    "\n",
    "    for idx_context, context in enumerate(full_context):  # iterate over each given context in the dev dataset\n",
    "        title = context[0]  # get the title of the context\n",
    "        if title not in [sf[0] for sf in supporting_facts]: continue  # skip if the title is not in the supporting facts\n",
    "        # because we only want to include the context if it is a supporting fact\n",
    "\n",
    "        string = title + '\\n' # start the context string with the title and a newline\n",
    "        for i, c in enumerate(context[1]): # iterate over each sentence in the context\n",
    "            string += c  # append the sentence to the context string\n",
    "\n",
    "        gold_context[question_id].append(string) # add the context string to the list for the currennt question id\n",
    "\n",
    "save_json('data/gold_context.json', gold_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T16:37:17.949944Z",
     "start_time": "2024-06-10T16:37:17.743388Z"
    }
   },
   "outputs": [],
   "source": [
    "hard_negatives = {}\n",
    "\n",
    "for question in _dev: # iterate over each question in the _dev dataset\n",
    "    question_id = question['_id'] # get the id of the question\n",
    "    if question_id not in keys: continue # skip if the question is not part of the ones from top10.json\n",
    "    \n",
    "    hard_negatives[question_id] = []  # initialize an empty list for the current question id in hard_negatives\n",
    "    supporting_facts = question['supporting_facts'] # get the supporting facts of the question\n",
    "\n",
    "    titles_top10 = [t.split('\\n')[0] for t in top10[question_id]] # get the titles from the top10 for the current question id\n",
    "\n",
    "    for idx_title, title in enumerate(titles_top10): # iterate over each title in the top10 titles\n",
    "        if title not in [sf[0] for sf in supporting_facts]: # if the title is not in the supporting facts\n",
    "            hard_negatives[question_id].append(top10[question_id][idx_title]) # we only want to include the context if it is not a\n",
    "            # supporting fact but seems useful according to the retriever\n",
    "\n",
    "save_json('data/hard_negatives.json', hard_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following cell loads the different contexts from the `.json` file and adds them to the `QA` class.\n",
    "I also generated a \"gibberish\" context which is just random characters to test the `QA.ask_by_mixed_context()` method.\n",
    "\n",
    "**IMPORTANT:** The structure of the final prompt is as follows:\n",
    "- Title of the context 1\n",
    "- linebreak\n",
    "- context 1 sentences without linebreaks\n",
    "- linebreak\n",
    "- linebreak\n",
    "- title of the context 2\n",
    "- linebreak\n",
    "- context 2 sentences without linebreaks\n",
    "- ...\n",
    "- linebreak\n",
    "- linebreak\n",
    "- question\n",
    "\n",
    "A short example of the functionality...\n",
    "\n",
    "First load the contexts and questions from the JSON files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:04:04.721354Z",
     "start_time": "2024-06-11T07:04:04.416349Z"
    }
   },
   "outputs": [],
   "source": [
    "hard_negatives = load_json('data/hard_negatives.json')\n",
    "gold_context = load_json('data/gold_context.json')\n",
    "top10 = load_json('data/results_10.json')\n",
    "# gibberish = load_json('project/json_thingy/gibberish.json') # load gibberish dict from json\n",
    "\n",
    "# this is to remove the double newlines in the contexts, that were added by matteo\n",
    "for key, value in top10.items():\n",
    "    for i, v in enumerate(value):\n",
    "        top10[key][i] = top10[key][i].replace('\\n\\n', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a `QA` object. this will load **ALL** questions in `dev.json`. The `model` argument specifies which openai model to use. The `system_prompt` argument is used to set the system prompt, which can also be done later with the `set_system_prompt()` method. `actually_prompt` is a boolean that determines if the model should be prompted or not. If set to `False`, the model just returns some dummy responses.\n",
    "\n",
    "the class internally works with the contexts and questions in the following way:\n",
    "\n",
    "when a `QA` instance is initialized, it loads `dev.json` questions from a JSON file into the `DevQuestions` object. this object stores each question as an instance of the nested `Question` class, wich includes the questions identifier, text and answer. contexts for these questions are stored in a dictionary within the `QA` instance, where each key is a question ID and each value is a list of context strings.\n",
    "\n",
    "the questions actually getting asked are always based on the contexts. The keys in the context dictionary tell the QA class which question to look up in the `DevQuestions` object. the corresponding contexts are then used to prompt the model.\n",
    "\n",
    "Use `add_context()` to add a new context to the QA object. The context should be a dictionary with question ids as keys and a list of context strings as values. it also takes a name for the context. for example, using\n",
    "\n",
    "```python\n",
    "qa.add_context(context_name='example_context', context={'question_idA': ['contextA1', 'contextA2'], 'question_idB': ['contextB1', 'contextB2']})\n",
    "```\n",
    "\n",
    "\n",
    "the `ask_by_context` asks questions based on a specific context. provide the name of the context, the number of questions to ask and the number of contexts to include in the prompt. If the number of questions or contexts `k` is not specified or is out of range the method uses all available questions / contexts. the prompts, responses and answers can be written to a .txt file for debugging and to check if everything works. for example, using\n",
    "\n",
    "```python\n",
    "results = qa.ask_by_context(context_name='example_context', num_questions=5, k=3, write_to_file=(True, 'output.txt'))\n",
    "```\n",
    "\n",
    "will ask 5 questions using the `example_context` including up to 3 contexts for each question (if theres less than that for a question, it will include this max. amount), and save the results to `output.txt`.\n",
    "\n",
    "the `ask_by_mixed_context` method combines contexts from two different context dictionaries. It uses question ids from `context_name1` and randomly adds contexts from `context_name2`. pass the number of questions to ask and the number of contexts to include from each dictionary (`k1`, `k2`). the method ensures that the contexts from both dictionaries are combined and shuffled before constructing the prompt. if needed, the prompts, responses, and answers can be saved to a file. again, if `k1` or `k2` are not specified or out of range, the method uses all available contexts. for example, using\n",
    "\n",
    "```python\n",
    "results = qa.ask_by_mixed_context(context_name1='example_context', context_name2='noisy_context', num_questions=5, k1=2, k2=2, write_to_file=(True, 'mixed_output.txt'))\n",
    "```\n",
    "\n",
    "will ask 5 questions specified by the ids in the `example_context` and add 2 contexts from it. additionaly it adds 2 random contexts from the `noisy_context` dictionary. the dictionary's keys should contain the same question ids.\n",
    "\n",
    "The `ask_by_context_without_context` just asks the questions without any context. Pretty lame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:04:09.915347Z",
     "start_time": "2024-06-11T07:04:08.532346Z"
    }
   },
   "outputs": [],
   "source": [
    "qa = QA('data/_dev.json', model='gpt-3.5-turbo', system_prompt='Q: ', actually_prompt=False)\n",
    "\n",
    "qa.set_system_prompt(\"For this task, you should provide a factoid answer. This means that you are limited to returning the exact answer to the question\"\n",
    "                          \"(which might be a person's name, a date, a place and so on), without any additional word and without putting the factoid answer in\"\n",
    "                          \"a sentence. For instance, if the question is \\\"Who was the lead singer of the rock band Queen?\\\", you should reply \\\"Freddy Mercury\\\",\"\n",
    "                          \"and not \\\"The lead singer of the band Queen was Freddy Mercury\\\".\", )\n",
    "\n",
    "qa.add_context(gold_context, 'gold_context')\n",
    "qa.add_context(hard_negatives, 'hard_negatives')\n",
    "qa.add_context(top10, 'top10')\n",
    "# qa.add_context(gibberish, 'gibberish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T06:37:34.736295Z",
     "start_time": "2024-06-11T06:37:34.718294Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|██████████| 10/10 [00:00<00:00, 9988.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: total adjustments of 'k': 10 of 10\n",
      "this warning is because the specified k is outside the valid range or exceeds available contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qa.actually_prompt = False\n",
    "a = qa.ask_by_context('gold_context', num_questions=10, k=0, write_to_file=(False, 'qa_output.txt'))\n",
    "a = qa.ask_by_context_without_context('gold_context', num_questions=10, write_to_file=(False, 'qa_output_nc.txt'))\n",
    "# a = qa.ask_by_mixed_context('top10', 'gibberish', num_questions=10, k1=3, k2=2, write_to_file=(False, 'qa_output_mixed.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T06:37:47.488997Z",
     "start_time": "2024-06-11T06:37:47.468999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_context\n",
      "hard_negatives\n",
      "top10\n"
     ]
    }
   ],
   "source": [
    "# you can also print the available contexts\n",
    "qa.print_context_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## METRICS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/69/a6/81d5dc9a612cf0c1810c2ebc4f2afddb900382276522b18d128213faeae3/pandas-2.2.2-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached pandas-2.2.2-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\matte\\onedrive - politecnico di milano\\poli\\erasmus\\corsi\\natural language processing\\group project\\nlpproject\\venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\matte\\onedrive - politecnico di milano\\poli\\erasmus\\corsi\\natural language processing\\group project\\nlpproject\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/9c/3d/a121f284241f08268b21359bd425f7d4825cffc5ac5cd0e1b3d82ffd2b10/pytz-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\matte\\onedrive - politecnico di milano\\poli\\erasmus\\corsi\\natural language processing\\group project\\nlpproject\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas # might be needed for next import"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T06:46:56.272865Z",
     "start_time": "2024-06-11T06:46:30.668868Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from project_work_group_12.evaluation import exact_match, bertscore "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T07:04:28.475501Z",
     "start_time": "2024-06-11T07:04:16.026502Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:09:19.467951Z",
     "start_time": "2024-06-11T07:08:57.793793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: num_questions must be between 1 and 1199! Setting num_questions to 1199.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:   3%|▎         | 38/1199 [00:20<10:11,  1.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m context_to_use \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtop10\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m K:\n\u001B[1;32m----> 7\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mqa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_by_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext_to_use\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_questions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwrite_to_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest/qa_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcontext_to_use\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_k\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mk\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     save_json(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest/qa_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcontext_to_use\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_k\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.json\u001B[39m\u001B[38;5;124m'\u001B[39m, result)\n",
      "Cell \u001B[1;32mIn[3], line 57\u001B[0m, in \u001B[0;36mQA.ask_by_context\u001B[1;34m(self, context_name, num_questions, k, write_to_file)\u001B[0m\n\u001B[0;32m     55\u001B[0m     prompts\u001B[38;5;241m.\u001B[39mappend(prompt)\n\u001B[0;32m     56\u001B[0m     answers\u001B[38;5;241m.\u001B[39mappend(question\u001B[38;5;241m.\u001B[39manswer)\n\u001B[1;32m---> 57\u001B[0m     responses\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactually_prompt \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdummy response! ;)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m write_to_file[\u001B[38;5;241m0\u001B[39m]: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_to_file(write_to_file[\u001B[38;5;241m1\u001B[39m], prompts, responses, answers)\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m k_adjustments:\n",
      "Cell \u001B[1;32mIn[3], line 183\u001B[0m, in \u001B[0;36mQA.ask\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prompt, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    181\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprompt must be a string!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 183\u001B[0m completion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msystem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msystem_prompt\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m completion\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmessage\u001B[38;5;241m.\u001B[39mcontent\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\openai\\_utils\\_utils.py:277\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 277\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\openai\\resources\\chat\\completions.py:590\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    560\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    588\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    589\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m--> 590\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    591\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    595\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    597\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    598\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    599\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    600\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    602\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    603\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    604\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    605\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    606\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    607\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    608\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    609\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    610\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    611\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    614\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    615\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    616\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    617\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    618\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    619\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    620\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    621\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    623\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    624\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\openai\\_base_client.py:1240\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1228\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1235\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1236\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1237\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1238\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1239\u001B[0m     )\n\u001B[1;32m-> 1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\openai\\_base_client.py:921\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    913\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    914\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    919\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    920\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    922\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    923\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\openai\\_base_client.py:952\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    949\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSending HTTP Request: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, request\u001B[38;5;241m.\u001B[39mmethod, request\u001B[38;5;241m.\u001B[39murl)\n\u001B[0;32m    951\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 952\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39msend(\n\u001B[0;32m    953\u001B[0m         request,\n\u001B[0;32m    954\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_stream_response_body(request\u001B[38;5;241m=\u001B[39mrequest),\n\u001B[0;32m    955\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    956\u001B[0m     )\n\u001B[0;32m    957\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    958\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered httpx.TimeoutException\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpx\\_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[1;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[0;32m    906\u001B[0m follow_redirects \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    907\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfollow_redirects\n\u001B[0;32m    908\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(follow_redirects, UseClientDefault)\n\u001B[0;32m    909\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m follow_redirects\n\u001B[0;32m    910\u001B[0m )\n\u001B[0;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[1;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    921\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpx\\_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[1;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[0;32m    939\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[0;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    948\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpx\\_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[1;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    977\u001B[0m     hook(request)\n\u001B[1;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    981\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpx\\_client.py:1015\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m   1010\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1011\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1012\u001B[0m     )\n\u001B[0;32m   1014\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[1;32m-> 1015\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1017\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n\u001B[0;32m   1019\u001B[0m response\u001B[38;5;241m.\u001B[39mrequest \u001B[38;5;241m=\u001B[39m request\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpx\\_transports\\default.py:233\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    220\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[0;32m    221\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    222\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    230\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[0;32m    231\u001B[0m )\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[1;32m--> 233\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[0;32m    238\u001B[0m     status_code\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mstatus,\n\u001B[0;32m    239\u001B[0m     headers\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m    240\u001B[0m     stream\u001B[38;5;241m=\u001B[39mResponseStream(resp\u001B[38;5;241m.\u001B[39mstream),\n\u001B[0;32m    241\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[0;32m    242\u001B[0m )\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    213\u001B[0m         closing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_requests_to_connections()\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[1;32m--> 216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, Iterable)\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    192\u001B[0m connection \u001B[38;5;241m=\u001B[39m pool_request\u001B[38;5;241m.\u001B[39mwait_for_connection(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[1;32m--> 196\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[0;32m    200\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[0;32m    204\u001B[0m     pool_request\u001B[38;5;241m.\u001B[39mclear_connection()\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_failed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[1;32m--> 101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:143\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_closed\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_closed()\n\u001B[1;32m--> 143\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:113\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs\n\u001B[0;32m    106\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[0;32m    107\u001B[0m     (\n\u001B[0;32m    108\u001B[0m         http_version,\n\u001B[0;32m    109\u001B[0m         status,\n\u001B[0;32m    110\u001B[0m         reason_phrase,\n\u001B[0;32m    111\u001B[0m         headers,\n\u001B[0;32m    112\u001B[0m         trailing_data,\n\u001B[1;32m--> 113\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_receive_response_headers(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    114\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    115\u001B[0m         http_version,\n\u001B[0;32m    116\u001B[0m         status,\n\u001B[0;32m    117\u001B[0m         reason_phrase,\n\u001B[0;32m    118\u001B[0m         headers,\n\u001B[0;32m    119\u001B[0m     )\n\u001B[0;32m    121\u001B[0m network_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_stream\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:186\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_headers\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    183\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 186\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mResponse):\n\u001B[0;32m    188\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:224\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    221\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[1;32m--> 224\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_network_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    226\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    228\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    234\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[0;32m    235\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[0;32m    236\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Milano\\Poli\\Erasmus\\Corsi\\Natural Language Processing\\group project\\NLPProject\\venv\\lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[1;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[0;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[1;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1259\u001B[0m, in \u001B[0;36mSSLSocket.recv\u001B[1;34m(self, buflen, flags)\u001B[0m\n\u001B[0;32m   1255\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1256\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1257\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1258\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuflen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1260\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1261\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv(buflen, flags)\n",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1132\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[0;32m   1131\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1133\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SSLError \u001B[38;5;28;01mas\u001B[39;00m x:\n\u001B[0;32m   1134\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m SSL_ERROR_EOF \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuppress_ragged_eofs:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "K = [1, 3, 5]\n",
    "\n",
    "qa.actually_prompt = True\n",
    "context_to_use = 'top10'\n",
    "\n",
    "for k in K:\n",
    "    result = qa.ask_by_context(context_to_use, num_questions=0, k=k, write_to_file=(False, f'test/qa_{context_to_use}_k{k}.txt'))\n",
    "    save_json(f'test/qa_{context_to_use}_k{k}.json', result)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match with k=1: 0.6\n",
      "Bertscore with k=1: 0.7213021427392959\n",
      "Exact match with k=3: 0.3\n",
      "Bertscore with k=3: 0.5079012331552804\n",
      "Exact match with k=5: 0.3\n",
      "Bertscore with k=5: 0.49595814319327475\n"
     ]
    }
   ],
   "source": [
    "for k in [1,3,5]:    \n",
    "    r = load_json(f'test/qa_top10_k{k}.json')\n",
    "    \n",
    "    print(f\"Exact match with k={k}:\", exact_match(r['responses'], r['answers']))\n",
    "    print(f\"Bertscore with k={k}:\", bertscore(r['answers'], r['responses']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T07:08:29.945905Z",
     "start_time": "2024-06-11T07:08:27.149905Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOLD CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T17:51:59.623086Z",
     "start_time": "2024-06-10T17:51:59.420917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|██████████| 10/10 [00:00<00:00, 9508.74it/s]\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 118-119: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeEncodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m context_to_use \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgold_context\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m K:\n\u001B[1;32m----> 7\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mqa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mask_by_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext_to_use\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_questions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwrite_to_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest/qa_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcontext_to_use\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_k\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mk\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     save_json(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest/qa_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcontext_to_use\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_k\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.json\u001B[39m\u001B[38;5;124m'\u001B[39m, result)\n",
      "Cell \u001B[1;32mIn[2], line 59\u001B[0m, in \u001B[0;36mQA.ask_by_context\u001B[1;34m(self, context_name, num_questions, k, write_to_file)\u001B[0m\n\u001B[0;32m     56\u001B[0m     answers\u001B[38;5;241m.\u001B[39mappend(question\u001B[38;5;241m.\u001B[39manswer)\n\u001B[0;32m     57\u001B[0m     responses\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mask(prompt) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactually_prompt \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdummy response! ;)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m write_to_file[\u001B[38;5;241m0\u001B[39m]: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_write_to_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrite_to_file\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponses\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43manswers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m k_adjustments:\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWARNING: total adjustments of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mk\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk_adjustments\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(prompts)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[2], line 274\u001B[0m, in \u001B[0;36mQA._write_to_file\u001B[1;34m(self, file_path, prompts, responses, answers)\u001B[0m\n\u001B[0;32m    272\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m    273\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(prompts)):\n\u001B[1;32m--> 274\u001B[0m         \u001B[43mfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrompt \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m:\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mprompts\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    275\u001B[0m         file\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mresponses[i]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    276\u001B[0m         file\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGround Truth \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00manswers[i]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\encodings\\cp1252.py:19\u001B[0m, in \u001B[0;36mIncrementalEncoder.encode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mencode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcodecs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharmap_encode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43mencoding_table\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mUnicodeEncodeError\u001B[0m: 'charmap' codec can't encode characters in position 118-119: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "K = [1, 3, 5]\n",
    "\n",
    "qa.actually_prompt = False\n",
    "context_to_use = 'gold_context'\n",
    "\n",
    "for k in K:\n",
    "    result = qa.ask_by_context(context_to_use, num_questions=10, k=k, write_to_file=(True, f'test/qa_{context_to_use}_k{k}.txt'))\n",
    "    save_json(f'test/qa_{context_to_use}_k{k}.json', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|██████████| 10/10 [00:00<00:00, 53498.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: total adjustments of 'k2': 10 of 10\n",
      "this warning is because the specified k2 is outside the valid range or exceeds available contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask_by_mixed_context('top10', 'gibberish', num_questions=10, k1=3, k2=12, write_to_file=(True, 'test/qa_mixed.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
